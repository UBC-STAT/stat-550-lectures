[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lecture slides",
    "section": "",
    "text": "Syllabus and introduction\nGit and Github\nOrganization\nGiving talks\nUnit tests\nBootstrap\nCluster computing\nTime series\nCausal inference\nReading statistical papers"
  },
  {
    "objectID": "slides/bootstrap.html#section",
    "href": "slides/bootstrap.html#section",
    "title": "The bootstrap",
    "section": "",
    "text": "\\[\\newcommand{\\Expect}[1]{E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\argmin}{\\arg\\min}\n\\newcommand{\\argmax}{\\arg\\max}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\P}{Pr}\n\\renewcommand{\\hat}{\\widehat}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\y}{\\mathbf{y}}\\]"
  },
  {
    "objectID": "slides/bootstrap.html#in-statistics",
    "href": "slides/bootstrap.html#in-statistics",
    "title": "The bootstrap",
    "section": "In statistics…",
    "text": "In statistics…\nThe “bootstrap” works. And well.\nIt’s good for “second-level” analysis.\n\n“First-level” analyses are things like \\(\\hat\\beta\\), \\(\\hat y\\), an estimator of the center (a median), etc.\n“Second-level” are things like \\(\\Var{\\hat\\beta}\\), a confidence interval for \\(\\hat y\\), or a median, etc.\n\nYou usually get these “second-level” properties from “the sampling distribution of an estimator”\n\nBut what if you don’t know the sampling distribution? Or you’re skeptical of the CLT argument?"
  },
  {
    "objectID": "slides/bootstrap.html#refresher-on-sampling-distributions",
    "href": "slides/bootstrap.html#refresher-on-sampling-distributions",
    "title": "The bootstrap",
    "section": "Refresher on sampling distributions",
    "text": "Refresher on sampling distributions\n\nIf \\(X_i\\) are iid Normal \\((0,\\sigma^2)\\), then \\(\\Var{\\bar{X}} = \\sigma^2 / n\\).\nIf \\(X_i\\) are iid and \\(n\\) is big, then \\(\\Var{\\bar{X}} \\approx \\Var{X_1} / n\\).\nIf \\(X_i\\) are iid Binomial \\((m, p)\\), then \\(\\Var{\\bar{X}} = mp(1-p) / n\\)"
  },
  {
    "objectID": "slides/bootstrap.html#example-of-unknown-sampling-distribution",
    "href": "slides/bootstrap.html#example-of-unknown-sampling-distribution",
    "title": "The bootstrap",
    "section": "Example of unknown sampling distribution",
    "text": "Example of unknown sampling distribution\nI estimate a LDA on some data.\nI get a new \\(x_0\\) and produce \\(\\hat{Pr}(y_0 =1 \\given x_0)\\).\nCan I get a 95% confidence interval for \\(Pr(y_0=1 \\given x_0)\\)?\n\nThe bootstrap gives this to you."
  },
  {
    "objectID": "slides/bootstrap.html#procedure",
    "href": "slides/bootstrap.html#procedure",
    "title": "The bootstrap",
    "section": "Procedure",
    "text": "Procedure\n\nResample your training data w/ replacement.\nCalculate a LDA on this sample.\nProduce a new prediction, call it \\(\\widehat{Pr}_b(y_0 =1 \\given x_0)\\).\nRepeat 1-3 \\(b = 1,\\ldots,B\\) times.\nCI: \\(\\left[2\\widehat{Pr}(y_0 =1 \\given x_0) - \\widehat{F}_{boot}(1-\\alpha/2),\\ 2\\widehat{Pr}(y_0 =1 \\given x_0) - \\widehat{F}_{boot}(\\alpha/2)\\right]\\)\n\n\\(\\hat{F}\\) is the “empirical” distribution of the bootstraps."
  },
  {
    "objectID": "slides/bootstrap.html#very-basic-example",
    "href": "slides/bootstrap.html#very-basic-example",
    "title": "The bootstrap",
    "section": "Very basic example",
    "text": "Very basic example\n\nLet \\(X_i\\sim Exponential(1/5)\\). The pdf is \\(f(x) = \\frac{1}{5}e^{-x/5}\\)\nI know if I estimate the mean with \\(\\bar{X}\\), then by the CLT (if \\(n\\) is big),\n\n\\[\\frac{\\sqrt{n}(\\bar{X}-E[X])}{s} \\approx N(0, 1).\\]\n\nThis gives me a 95% confidence interval like \\[\\bar{X} \\pm 2 \\frac{s}{\\sqrt{n}}\\]\nBut I don’t want to estimate the mean, I want to estimate the median."
  },
  {
    "objectID": "slides/bootstrap.html#now-what",
    "href": "slides/bootstrap.html#now-what",
    "title": "The bootstrap",
    "section": "Now what",
    "text": "Now what\n\n\n\n\nI give you a sample of size 500, you give me the sample median.\nHow do you get a CI?\nYou can use the bootstrap!\n\n\nset.seed(2022-11-01)\nx <- rexp(n, 1 / 5)\n(med <- median(x)) # sample median\n\n[1] 3.669627\n\nB <- 100\nalpha <- 0.05\nbootMed <- function() median(sample(x, replace = TRUE)) # resample, and get the median\nFhat <- replicate(B, bootMed()) # repeat B times, \"empirical distribution\"\nCI <- 2 * med - quantile(Fhat, probs = c(1 - alpha / 2, alpha / 2))"
  },
  {
    "objectID": "slides/bootstrap.html#slightly-harder-example",
    "href": "slides/bootstrap.html#slightly-harder-example",
    "title": "The bootstrap",
    "section": "Slightly harder example",
    "text": "Slightly harder example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = Hwt ~ 0 + Bwt, data = fatcats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9293 -1.0460 -0.1407  0.8298 16.2536 \n\nCoefficients:\n    Estimate Std. Error t value Pr(>|t|)    \nBwt  3.81895    0.07678   49.74   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.549 on 143 degrees of freedom\nMultiple R-squared:  0.9454,    Adjusted R-squared:  0.945 \nF-statistic:  2474 on 1 and 143 DF,  p-value: < 2.2e-16\n\n\n       2.5 %  97.5 %\nBwt 3.667178 3.97073"
  },
  {
    "objectID": "slides/bootstrap.html#when-we-fit-models-we-examine-diagnostics",
    "href": "slides/bootstrap.html#when-we-fit-models-we-examine-diagnostics",
    "title": "The bootstrap",
    "section": "When we fit models, we examine diagnostics",
    "text": "When we fit models, we examine diagnostics\n\n\n\n\n\n\n\n\n\n\n\nThe tails are too fat, I don’t believe that CI…\n\nWe bootstrap\n\nB <- 500\nbhats <- double(B)\nalpha <- .05\nfor (b in 1:B) {\n  samp <- sample(1:nrow(fatcats), replace = TRUE)\n  newcats <- fatcats[samp, ] # new data\n  bhats[b] <- coef(lm(Hwt ~ 0 + Bwt, data = newcats)) \n}\n\n2 * coef(cats.lm) - # Bootstrap CI\n  quantile(bhats, probs = c(1 - alpha / 2, alpha / 2))\n\n   97.5%     2.5% \n3.654977 3.955927 \n\nconfint(cats.lm) # Original CI\n\n       2.5 %  97.5 %\nBwt 3.667178 3.97073"
  },
  {
    "objectID": "slides/bootstrap.html#an-alternative",
    "href": "slides/bootstrap.html#an-alternative",
    "title": "The bootstrap",
    "section": "An alternative",
    "text": "An alternative\n\nSo far, I didn’t use any information about the data-generating process.\nWe’ve done the non-parametric bootstrap\nThis is easiest, and most common for most cases.\n\n\nBut there’s another version\n\nYou could try a “parametric bootstrap”\nThis assumes knowledge about the DGP"
  },
  {
    "objectID": "slides/bootstrap.html#same-data",
    "href": "slides/bootstrap.html#same-data",
    "title": "The bootstrap",
    "section": "Same data",
    "text": "Same data\n\n\nNon-parametric bootstrap\nSame as before\n\nB <- 500\nbhats <- double(B)\nalpha <- .05\nfor (b in 1:B) {\n  samp <- sample(1:nrow(fatcats), replace = TRUE)\n  newcats <- fatcats[samp, ] # new data\n  bhats[b] <- coef(lm(Hwt ~ 0 + Bwt, data = newcats)) \n}\n\n2 * coef(cats.lm) - # NP Bootstrap CI\n  quantile(bhats, probs = c(1-alpha/2, alpha/2))\n\n   97.5%     2.5% \n3.673559 3.970251 \n\nconfint(cats.lm) # Original CI\n\n       2.5 %  97.5 %\nBwt 3.667178 3.97073\n\n\n\nParametric bootstrap\n\nAssume that the linear model is TRUE.\nThen, \\(\\texttt{Hwt}_i = \\widehat{\\beta}\\times \\texttt{Bwt}_i + \\widehat{e}_i\\), \\(\\widehat{e}_i \\approx \\epsilon_i\\)\nThe \\(\\epsilon_i\\) is random \\(\\longrightarrow\\) just resample \\(\\widehat{e}_i\\).\n\n\nB <- 500\nbhats <- double(B)\nalpha <- .05\ncats.lm <- lm(Hwt ~ 0 + Bwt, data = fatcats)\nnewcats <- fatcats\nfor (b in 1:B) {\n  samp <- sample(residuals(cats.lm), replace = TRUE)\n  newcats$Hwt <- predict(cats.lm) + samp # new data\n  bhats[b] <- coef(lm(Hwt ~ 0 + Bwt, data = newcats)) \n}\n\n2 * coef(cats.lm) - # Parametric Bootstrap CI\n  quantile(bhats, probs = c(1 - alpha/2, alpha/2))\n\n   97.5%     2.5% \n3.665531 3.961896"
  },
  {
    "objectID": "slides/bootstrap.html#bootstrap-error-sources",
    "href": "slides/bootstrap.html#bootstrap-error-sources",
    "title": "The bootstrap",
    "section": "Bootstrap error sources",
    "text": "Bootstrap error sources\nSimulation error:\nusing only \\(B\\) samples to estimate \\(F\\) with \\(\\hat{F}\\).\nStatistical error:\nour data depended on a sample from the population. We don’t have the whole population so we make an error by using a sample\n(Note: this part is what always happens with data, and what the science of statistics analyzes.)\nSpecification error:\nIf we use the parametric bootstrap, and our model is wrong, then we are overconfident."
  },
  {
    "objectID": "slides/cluster-computing.html#ubc-hpc",
    "href": "slides/cluster-computing.html#ubc-hpc",
    "title": "Cluster computing (at UBC)",
    "section": "UBC HPC",
    "text": "UBC HPC\n3 potentially useful systems:\n\nDepartment VM\nUBC ARC Sockeye\nCompute Canada\n\nI’ve only used 1 and 3. I mainly use 3.\nAccessing\nAs far as I know, access for students requires “faculty” support\n\nEmail The/Binh.\nPossible you can access without a faculty PI.\nEmail your advisor to ask for an account.\n\nThe rest of this will focus on 3."
  },
  {
    "objectID": "slides/cluster-computing.html#prerequisites",
    "href": "slides/cluster-computing.html#prerequisites",
    "title": "Cluster computing (at UBC)",
    "section": "Prerequisites",
    "text": "Prerequisites\n\n\n\nCommand line interface (Terminal on Mac)\n(optional) helpful to have ftp client. (Cyberduck)\nGlobus Connect. File transfer approved by CC.\n\n\nUseful CL commands\n\ncd ~/path/to/directory\n\ncp file/to/copy.txt duplicated/as/copy1.txt\n\nrm file/to/delete.txt\n\nrm -r dir/to/delete/\n\nls -a # list all files"
  },
  {
    "objectID": "slides/cluster-computing.html#how-to-connect",
    "href": "slides/cluster-computing.html#how-to-connect",
    "title": "Cluster computing (at UBC)",
    "section": "How to connect",
    "text": "How to connect\nLogin to a system:\n\nssh dajmcdon@cedar.computecanada.ca\n\n\nUpon login, you’re on a “head” or “login” node.\nJobs > 30min will be killed.\nYou can continuously run short interactive jobs."
  },
  {
    "objectID": "slides/cluster-computing.html#rule-1",
    "href": "slides/cluster-computing.html#rule-1",
    "title": "Cluster computing (at UBC)",
    "section": "Rule 1",
    "text": "Rule 1\n\n\n\n\n\n\nTip\n\n\nIf you’re doing work for school: run it on one of these machines.\n\n\n\n\nYes, there is overhead to push data over and pull results back.\nBut CC/Sockeye is much faster than your machine.\nAnd this won’t lock up your laptop for 4 hours while you run the job.\nIt’s also a good experience.\nYou can log out and leave the job running. Just log back in to see if it’s done (you should always have some idea how long it will take)"
  },
  {
    "objectID": "slides/cluster-computing.html#modules",
    "href": "slides/cluster-computing.html#modules",
    "title": "Cluster computing (at UBC)",
    "section": "Modules",
    "text": "Modules\n\nOnce you connect with ssh:\nThere are no Applications loaded.\nYou must tell the system what you want.\nThe command is module load r or module load sas\nIf you find yourself using the same modules all the time:\n\n\nmodule load StdEnv/2020 r gurobi python # stuff I use\n\nmodule save my_modules # save loaded modules\n\nmodule restore my_modules # on login, load the usual set"
  },
  {
    "objectID": "slides/cluster-computing.html#running-something-interactively",
    "href": "slides/cluster-computing.html#running-something-interactively",
    "title": "Cluster computing (at UBC)",
    "section": "Running something interactively",
    "text": "Running something interactively\n\nLogin\nLoad modules\nRequest interactive compute\n\n\nsalloc --time=1:0:0 --ntasks=1 --account=def-dajmcdon --mem-per-cpu=4096M\n\nThis says:\n\nAllocate 1 hour\nOn 1 CPU\nFor the user def-dajmcdon (that’s me, accounts start with def-)\n4GB of RAM\n\nThen I would start R\n\nr\n\nAnd run whatever I want. If it takes more than an hour or needs more than 4GB of memory, it’ll quit."
  },
  {
    "objectID": "slides/cluster-computing.html#interactive-jobs",
    "href": "slides/cluster-computing.html#interactive-jobs",
    "title": "Cluster computing (at UBC)",
    "section": "Interactive jobs",
    "text": "Interactive jobs\n\nOnce started they’ll just go\nYou can do whatever else you want on your machine\nBut you can’t kill the connection\nSo don’t close your laptop and walk away\nThis is not typically the best use of this resource.\nBetter is likely syzygy.\n\nAlthough, syzygy has little memory and little storage, so it won’t do intensive tasks\nI think your home dir is limited to 1GB"
  },
  {
    "objectID": "slides/cluster-computing.html#big-memory-jobs",
    "href": "slides/cluster-computing.html#big-memory-jobs",
    "title": "Cluster computing (at UBC)",
    "section": "Big memory jobs",
    "text": "Big memory jobs\n\nPossible you can do this interactively, but discouraged\n\n\n\n\n\n\n\nExample\n\n\n\nNeuroscience project\nDataset is about 10GB\nPeak memory usage during analysis is about 24GB\nCan’t do this on my computer\nWant to offload onto CC.ca\n\n\n\n\n\nWrite a R/python script that does the whole analysis and saves the output.\nYou need to ask CC to run the script for you."
  },
  {
    "objectID": "slides/cluster-computing.html#the-scheduler",
    "href": "slides/cluster-computing.html#the-scheduler",
    "title": "Cluster computing (at UBC)",
    "section": "The scheduler",
    "text": "The scheduler\n\nWhile you can log in to CC and “do stuff”\nResources are limited.\nThere’s a process that determines who gets resources when.\nTechnically the salloc command we used before requested some resources.\nIt may “sit” until the resources you want are available, but probably not long.\nAnything else has to go through the scheduler.\nCompute Canada uses the slurm scheduler"
  },
  {
    "objectID": "slides/cluster-computing.html#example-script",
    "href": "slides/cluster-computing.html#example-script",
    "title": "Cluster computing (at UBC)",
    "section": "Example script",
    "text": "Example script\n\n#!/bin/bash\n\n#SBATCH --account=def-dajmcdon\n#SBATCH --job-name=dlbcl-suffpcr\n#SBATCH --output=%x-%j.out\n#SBATCH --error=%x-%j.out\n#SBATCH --time=10:00:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=32G\n\nRscript -e 'source(\"dlbcl-nocv.R\")'\n\n\nThis asks for 10 hours of compute time with 32GB of memory\nThe job-name/output/error fields are for convenience.\nIf unspecified, I’ll end up with files named things like jobid60607-60650934.out"
  },
  {
    "objectID": "slides/cluster-computing.html#submitting-and-other-useful-commands",
    "href": "slides/cluster-computing.html#submitting-and-other-useful-commands",
    "title": "Cluster computing (at UBC)",
    "section": "Submitting and other useful commands",
    "text": "Submitting and other useful commands\n\nSuppose that slurm script is saved as dlbcl-slurm.sh\n\n\nsbatch dlbcl-slurm.sh # submit the job to the scheduler\n\nsqueue -u $USER # show status of your jobs ($USER is an env variable)\n\nscancel -u $USER # cancel all your jobs\n\nscancel -t PENDING -u $USER # cancel all your pending jobs\n\n\n\n\n\n\n\nImportant\n\n\n\nJobs inherit environment variables. So if you load modules, then submit, your modules are available to run.\nOn Cedar, jobs cannot run from ~/. It must be run from ~/scratch/ or ~/projects/."
  },
  {
    "objectID": "slides/cluster-computing.html#types-of-jobs",
    "href": "slides/cluster-computing.html#types-of-jobs",
    "title": "Cluster computing (at UBC)",
    "section": "Types of jobs",
    "text": "Types of jobs\n\nBig jobs (need lots of RAM)\nGPU jobs (you want deep learning, I don’t know how to do this)\nOther jobs with internal parallelism (I almost never do this)\nEmbarrassingly parallel jobs (I do this all the time)"
  },
  {
    "objectID": "slides/cluster-computing.html#simple-parallelization",
    "href": "slides/cluster-computing.html#simple-parallelization",
    "title": "Cluster computing (at UBC)",
    "section": "Simple parallelization",
    "text": "Simple parallelization\n\nMost of my major computing needs are “embarrassingly parallel”\nI want to run a few algorithms on a bunch of different simulated datasets under different parameter configurations.\nPerhaps run the algos on some real data too.\nR has packages which are good for parallelization (snow, snowfall, Rmpi, parallel)\nThis is how I originally learned to do parallel computing. But these packages are not good for the cluster\nThey’re fine for your machine, but we’ve already decided we’re not going to do that anymore."
  },
  {
    "objectID": "slides/cluster-computing.html#example-of-the-bad-parallelism",
    "href": "slides/cluster-computing.html#example-of-the-bad-parallelism",
    "title": "Cluster computing (at UBC)",
    "section": "Example of the bad parallelism",
    "text": "Example of the bad parallelism\nTorque script\n\n#!/bin/bash  \n#PBS -l nodes=8:ppn=8,walltime=200:00:00\n#PBS -m abe\n#PBS -n ClusterPermute \n#PBS -j oe \n\nmpirun -np 64 -machinefile $PBS_NODEFILE R CMD BATCH ClusterPermute.R\n\n\nTorque is a different scheduler. UBC ARC Sockeye uses Torque.\nLooks much like Slurm.\nHere, ClusterPermute.R uses Rmpi to do “parallel lapply”\nSo I asked for 8 processors on each of 8 nodes.\n\n\nProblem\n\nThe scheduler has to find 8 nodes with 8 available processors before this job will start.\nThis often takes a while, sometimes days.\nBut the code doesn’t need all those things to happen at the same time because the jobs don’t interact."
  },
  {
    "objectID": "slides/cluster-computing.html#batchtools",
    "href": "slides/cluster-computing.html#batchtools",
    "title": "Cluster computing (at UBC)",
    "section": "{batchtools}",
    "text": "{batchtools}\n\nUsing R (or python) to parallelize is inefficient when there’s a scheduler in the middle.\nBetter is to actually submit 64 different jobs each requiring 1 node\nThen each can get out of the queue whenever a processor becomes available.\nBut that would seem to require writing 64 different slurm scripts\n{batchtools} does this for you, all in R\n\nIt automates writing/submitting slurm/torque scripts.\nIt automatically stores output, and makes it easy to collect.\nIt generates lots of jobs.\nAll this from R directly.\n\n\nIt’s easy to port across machines/schedulers.\nI can test parts (or even run) it on my machine without making changes for the cluster."
  },
  {
    "objectID": "slides/cluster-computing.html#setup-batchtools",
    "href": "slides/cluster-computing.html#setup-batchtools",
    "title": "Cluster computing (at UBC)",
    "section": "Setup {batchtools}",
    "text": "Setup {batchtools}\n\nCreate a directory where all your jobs will live (in subdirectories). Mine is ~/\nIn that directory, you need a template file. Mine is ~/.batchtools.slurm.tmpl (next slide)\nCreate a configuration file which lives in your home directory. You must name it ~/.batchtools.conf.R.\n\n\n# ~/.batchtools.conf.R\ncluster.functions = makeClusterFunctionsSlurm()"
  },
  {
    "objectID": "slides/cluster-computing.html#batchtools.slurm.tmpl",
    "href": "slides/cluster-computing.html#batchtools.slurm.tmpl",
    "title": "Cluster computing (at UBC)",
    "section": "~/.batchtools.slurm.tmpl",
    "text": "~/.batchtools.slurm.tmpl\n\n#!/bin/bash\n\n## Job Resource Interface Definition\n##\n## ntasks [integer(1)]:       Number of required tasks,\n##                            Set larger than 1 if you want to further parallelize\n##                            with MPI within your job.\n## ncpus [integer(1)]:        Number of required cpus per task,\n##                            Set larger than 1 if you want to further parallelize\n##                            with multicore/parallel within each task.\n## walltime [integer(1)]:     Walltime for this job, in seconds.\n##                            Must be at least 60 seconds for Slurm to work properly.\n## memory   [integer(1)]:     Memory in megabytes for each cpu.\n##                            Must be at least 100 (when I tried lower values my\n##                            jobs did not start at all).\n##\n## Default resources can be set in your .batchtools.conf.R by defining the variable\n## 'default.resources' as a named list.\n\n<%\n# relative paths are not handled well by Slurm\nlog.file = fs::path_expand(log.file)\n-%>\n\n#SBATCH --account=def-dajmcdon\n#SBATCH --mail-user=daniel@stat.ubc.ca\n#SBATCH --mail-type=ALL\n#SBATCH --job-name=<%= job.name %>\n#SBATCH --output=<%= log.file %>\n#SBATCH --error=<%= log.file %>\n#SBATCH --time=<%= resources$walltime %>\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=<%= resources$ncpus %>\n#SBATCH --mem-per-cpu=<%= resources$memory %>\n<%= if (array.jobs) sprintf(\"#SBATCH --array=1-%i\", nrow(jobs)) else \"\" %>\n\n## Run R:\n## we merge R output with stdout from SLURM, which gets then logged via --output option\nRscript -e 'batchtools::doJobCollection(\"<%= uri %>\")'\n\n\nWhen I’m ready to run, I’ll call something like:\n\nbatchtools::submitJobs(job.ids, resources = list(ncpus=1, walltime=\"24:00:00\", memory=\"32G\"))"
  },
  {
    "objectID": "slides/cluster-computing.html#workflow",
    "href": "slides/cluster-computing.html#workflow",
    "title": "Cluster computing (at UBC)",
    "section": "Workflow",
    "text": "Workflow\nSee the vignette: vignette(\"batchtools\")\nor the\nwebsite\n\nCreate a folder to hold your code. Mine usually contains 2 files, one to set up/run the experiment, one to collect results. Code needed to run the experiment lives in an R package.\nWrite a script to setup the experiment and submit.\nWait.\nCollect your results. Copy back to your machine etc."
  },
  {
    "objectID": "slides/cluster-computing.html#example-1-use-genetics-data-to-viral-load",
    "href": "slides/cluster-computing.html#example-1-use-genetics-data-to-viral-load",
    "title": "Cluster computing (at UBC)",
    "section": "Example 1: Use genetics data to viral load",
    "text": "Example 1: Use genetics data to viral load\n\nAn “extra” example in a methods paper to appease reviewers\nMethod is:\n\napply a special version of PCA to a big (wide) data set\nDo OLS using the top few PCs\n\nThis is “principle components regression” with sparse principle components.\nGot 413 COVID patients, measure “viral load” and gene expression\n9435 differentially expressed genes.\nThe method needs to form a 10K x 10K matrix multiple times and do an approximate SVD. Requires 32GB memory. Compute time is ~6 hours.\nTwo tuning parameters: \\(\\lambda\\) and number of PCs\nWant to do CV to choose, and then use those on the whole data, describe selected genes."
  },
  {
    "objectID": "slides/cluster-computing.html#example-1-use-genetics-data-to-viral-load-1",
    "href": "slides/cluster-computing.html#example-1-use-genetics-data-to-viral-load-1",
    "title": "Cluster computing (at UBC)",
    "section": "Example 1: Use genetics data to viral load",
    "text": "Example 1: Use genetics data to viral load\n\nlibrary(batchtools)\n\nreg <- makeExperimentRegistry(\"spcr-genes\", packages = c(\"tidyverse\", \"suffpcr\"))\nx <- readRDS(here::here(\"suffpcr-covid\", \"covid_x.rds\"))\ny <- readRDS(here::here(\"suffpcr-covid\", \"covid_y.rds\"))\n\nsubsample = function(data, job, ratio, ...) {\n  n = nrow(data$x)\n  train = sample(n, floor(n * ratio))\n  test = setdiff(seq_len(n), train)\n  list(test = test, train = train)\n}\n\naddProblem(\"cv\", data = list(x=x, y=y), fun = subsample)\naddProblem(\"full\", data = list(x=x, y=y))\n\naddAlgorithm(\n  \"spcr_cv\", \n  fun = function(job, data, instance,...) {\n    fit <- suffpcr(data$x[instance$train,], data$y[instance$train], lambda_min = 0, lambda_max = 1, ...)\n    valid_err <- colMeans(\n      (data$y[instance$test] - as.matrix(predict(fit, newdata = data$x[instance$test,])))^2, \n      na.rm=TRUE\n    )\n    return(list(fit = fit, valid_err = valid_err))\n  }\n)\n\naddAlgorithm(\n  \"spcr_full\",\n  fun = function(job, data, instance,...) suffpcr(data$x, data$y, lambda_max = 1, lambda_min = 0, ...)\n)\n\npdes_cv <- list(cv = data.frame(ratio = .75))\npdes_full <- list(full = data.frame())\nades_cv <- list(spcr_cv = data.frame(d = c(3, 5, 15)))\nades_full <- list(spcr_full = data.frame(d = c(3, 5, 15)))\naddExperiments(pdes_cv, ades_cv, repls = 5L)\naddExperiments(pdes_full, ades_full)\n\nsubmitJobs(findJobs(), resources = list(ncpus = 1, walltime = \"8:00:00\", memory = \"32G\"))\n\nEnd up with 18 jobs."
  },
  {
    "objectID": "slides/cluster-computing.html#example-2-predicting-future-covid-cases",
    "href": "slides/cluster-computing.html#example-2-predicting-future-covid-cases",
    "title": "Cluster computing (at UBC)",
    "section": "Example 2: Predicting future COVID cases",
    "text": "Example 2: Predicting future COVID cases\n\nTake a few very simple models and demonstrate that some choices make a big difference in accuracy.\nAt each time \\(t\\), download COVID cases as observed on day \\(t\\) for a bunch of locations\nEstimate a few different models for predicting days \\(t+1,\\ldots,t+k\\)\nStore point and interval forecasts.\nDo this for \\(t\\) every week over a year."
  },
  {
    "objectID": "slides/cluster-computing.html#example-2-predicting-future-covid-cases-1",
    "href": "slides/cluster-computing.html#example-2-predicting-future-covid-cases-1",
    "title": "Cluster computing (at UBC)",
    "section": "Example 2: Predicting future COVID cases",
    "text": "Example 2: Predicting future COVID cases\n\nfcasters <- list.files(here::here(\"code\", \"forecasters\"))\nfor (fcaster in fcasters) source(here::here(\"code\", \"forecasters\", fcaster))\nregistry_path <- here::here(\"data\", \"forecast-experiments\")\nsource(here::here(\"code\", \"common-pars.R\"))\n\n# Setup the data ----------------------------------------------------\nreg <- makeExperimentRegistry(\n  registry_path,\n  packages = c(\"tidyverse\", \"covidcast\"),\n  source = c(here::here(\"code\", \"forecasters\", fcasters), here::here(\"code\", \"common-pars.R\"))\n)\n\ngrab_data <- function(data, job, forecast_date, ...) {\n  dat <- covidcast_signals(\n    data_sources, signals, as_of = forecast_date, \n    geo_type = geo_type, start_day = \"2020-04-15\") %>% \n    aggregate_signals(format = \"wide\") \n  names(dat)[3:5] <- c(\"value\", \"num\", \"covariate\") # assumes 2 signals\n  dat %>% \n    filter(!(geo_value %in% drop_geos)) %>% \n    group_by(geo_value) %>% \n    arrange(time_value)\n}\naddProblem(\"covidcast_proper\", fun = grab_data, cache = TRUE)\n\n# Algorithm wrappers -----------------------------------------------------\nbaseline <- function(data, job, instance, ...) {\n  instance %>% \n    dplyr::select(geo_value, value) %>% \n    group_modify(prob_baseline, ...)\n}\nar <- function(data, job, instance, ...) {\n  instance %>% \n    dplyr::select(geo_value, time_value, value) %>% \n    group_modify(prob_ar, ...)\n}\nqar <- function(data, job, instance, ...) {\n  instance %>% \n    dplyr::select(geo_value, time_value, value) %>% \n    group_modify(quant_ar, ...)\n}\ngam <- function(data, job, instance, ...) {\n  instance %>% \n    dplyr::select(geo_value, time_value, value) %>%\n    group_modify(safe_prob_gam_ar, ...)\n}\nar_cov <- function(data, job, instance, ...) {\n  instance %>% \n    group_modify(prob_ar_cov, ...)\n}\njoint <- function(data, job, instance, ...) {\n  instance %>% \n    dplyr::select(geo_value, time_value, value) %>% \n    joint_ar(...)\n}\ncorrected_ar <- function(data, job, instance, ...) {\n  instance %>% \n    dplyr::select(geo_value, time_value, num) %>% \n    rename(value = num) %>% \n    corrections_single_signal(cparams) %>% \n    group_modify(prob_ar, ...)\n}\n\naddAlgorithm(\"baseline\", baseline)\naddAlgorithm(\"ar\", ar)\naddAlgorithm(\"qar\", qar)\naddAlgorithm(\"gam\", gam)\naddAlgorithm(\"ar_cov\", ar_cov)\naddAlgorithm(\"joint_ar\", joint)\naddAlgorithm(\"corrections\", corrected_ar)\n\n# Experimental design -----------------------------------------------------\nproblem_design <- list(covidcast_proper = data.frame(forecast_date = forecast_dates))\nalgorithm_design <- list(\n  baseline = CJ(train_window = train_windows, min_train_window = min(train_windows), ahead = aheads),\n  ar = CJ(\n    train_window = train_windows, min_train_window = min(train_windows), \n    lags = lags_list, ahead = aheads\n  ),\n  qar = CJ(\n    train_window = train_windows, min_train_window = min(train_windows),\n    lags = lags_list, ahead = aheads\n  ),\n  gam = CJ(\n    train_window = train_windows, min_train_window = min(train_windows),\n    lags = lags_list, ahead = aheads, df = gam_df\n  ),\n  ar_cov = CJ(\n    train_window = train_windows, min_train_window = min(train_windows), \n    lags = lags_list, ahead = aheads\n  ),\n  joint_ar = CJ(\n    train_window = joint_train_windows, min_train_window = min(joint_train_windows), \n    lags = lags_list, ahead = aheads\n  ),\n  corrections = CJ(\n    train_window = train_windows, min_train_window = min(train_windows),\n    lags = lags_list, ahead = aheads\n  )\n)\n\naddExperiments(problem_design, algorithm_design)\nids <- unwrap(getJobPars()) %>% \n  select(job.id, forecast_date) %>% \n  mutate(chunk = as.integer(as.factor(forecast_date))) %>% \n  select(-forecast_date)\n\n## ~13000 jobs, we don't want to submit that many since they run fast\n## Chunk them into groups by forecast_date (to download once for the group)\n## Results in 68 chunks\n\nsubmitJobs(ids, resources = list(ncpus = 1, walltime = \"4:00:00\", memory = \"16G\"))"
  },
  {
    "objectID": "slides/git.html#why-version-control",
    "href": "slides/git.html#why-version-control",
    "title": "Version control",
    "section": "Why version control?",
    "text": "Why version control?\n\n\n\n\n\n\nMuch of this lecture is borrowed/stolen from Colin Rundel and Karl Broman"
  },
  {
    "objectID": "slides/git.html#why-version-control-1",
    "href": "slides/git.html#why-version-control-1",
    "title": "Version control",
    "section": "Why version control?",
    "text": "Why version control?\n\nSimple formal system for tracking all changes to a project\nTime machine for your projects\n\nTrack blame and/or praise\nRemove the fear of breaking things\n\nLearning curve is steep, but when you need it you REALLY need it\n\n\n\n\nWords of wisdom\n\n\nYour closest collaborator is you six months ago, but you don’t reply to emails.\n– Paul Wilson"
  },
  {
    "objectID": "slides/git.html#why-git",
    "href": "slides/git.html#why-git",
    "title": "Version control",
    "section": "Why Git",
    "text": "Why Git\n\n\n\nYou could use something like Box or Dropbox\nThese are poor-man’s version control\nGit is much more appropriate\nIt works with large groups\nIt’s very fast\nIt’s much better at fixing mistakes\nTech companies use it (so it’s in your interest to have some experience)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nThis will hurt, but what doesn’t kill you, makes you stronger."
  },
  {
    "objectID": "slides/git.html#overview",
    "href": "slides/git.html#overview",
    "title": "Version control",
    "section": "Overview",
    "text": "Overview\n\ngit is a command line program that lives on your machine\nIf you want to track changes in a directory, you type git init\nThis creates a (hidden) directory called .git\nThe .git directory contains a history of all changes made to “versioned” files\nThis top directory is referred to as a “repository” or “repo”\nhttp://github.com is a service that hosts a repo remotely and has other features: issues, project boards, pull requests, renders .ipynb & .md\nSome IDEs (pycharm, RStudio, VScode) have built in git\ngit/GitHub is broad and complicated. Here, just what you need"
  },
  {
    "objectID": "slides/git.html#aside-on-built-in-command-line",
    "href": "slides/git.html#aside-on-built-in-command-line",
    "title": "Version control",
    "section": "Aside on “Built-in” & “Command line”",
    "text": "Aside on “Built-in” & “Command line”\n\n\n\n\n\n\nTip\n\n\nFirst things first, RStudio and the Terminal\n\n\n\n\nCommand line is the “old” type of computing. You type commands at a prompt and the computer “does stuff”.\nYou may not have seen where this is. RStudio has one built in called “Terminal”\nThe Mac System version is also called “Terminal”. If you have a Linux machine, this should all be familiar.\nWindows is not great at this.\nTo get the most out of Git, you have to use the command line."
  },
  {
    "objectID": "slides/git.html#typical-workflow",
    "href": "slides/git.html#typical-workflow",
    "title": "Version control",
    "section": "Typical workflow",
    "text": "Typical workflow\n\nDownload a repo from Github\n\ngit clone https://github.com/stat550-2021/lecture-slides.git\n\nCreate a branch\n\ngit branch <branchname>\n\nMake changes to your files.\nAdd your changes to be tracked (“stage” them)\n\ngit add <name/of/tracked/file>\n\nCommit your changes\n\ngit commit -m \"Some explanatory message\"\nRepeat 3–5 as needed. Once you’re satisfied\n\nPush to Github\n\ngit push\ngit push -u origin <branchname>"
  },
  {
    "objectID": "slides/git.html#what-should-be-tracked",
    "href": "slides/git.html#what-should-be-tracked",
    "title": "Version control",
    "section": "What should be tracked?",
    "text": "What should be tracked?\n\nDefinitely\ncode, markdown documentation, tex files, bash scripts/makefiles, …\n\nPossibly\nlogs, jupyter notebooks, images (that won’t change), …\n\nQuestionable\nprocessed data, static pdfs, …\n\nDefinitely not\nfull data, continually updated pdfs, other things compiled from source code, …"
  },
  {
    "objectID": "slides/git.html#what-things-to-track",
    "href": "slides/git.html#what-things-to-track",
    "title": "Version control",
    "section": "What things to track",
    "text": "What things to track\n\nYou decide what is “versioned”.\nA file called .gitignore tells git files or types to never track\n\n# History files\n.Rhistory\n.Rapp.history\n\n# Session Data files\n.RData\n\n# User-specific files\n.Ruserdata\n\n# Compiled junk\n*.o\n*.so\n*.DS_Store\n\nShortcut to track everything (use carefully):\n\ngit add ."
  },
  {
    "objectID": "slides/git.html#rules",
    "href": "slides/git.html#rules",
    "title": "Version control",
    "section": "Rules",
    "text": "Rules\n\nEach team has their own repo\nMake a PR against main to submit\nPeer evaluations are done via PR review (also send to Estella)\nYOU must make at least 5 commits (fewer will lead to deductions)\nI review your work and merge the PR\n\nProject submissions are done similarly."
  },
  {
    "objectID": "slides/git.html#whats-a-pr",
    "href": "slides/git.html#whats-a-pr",
    "title": "Version control",
    "section": "What’s a PR?",
    "text": "What’s a PR?\n\nThis exists on Github (not git)\nDemonstration"
  },
  {
    "objectID": "slides/git.html#some-things-to-be-aware-of",
    "href": "slides/git.html#some-things-to-be-aware-of",
    "title": "Version control",
    "section": "Some things to be aware of",
    "text": "Some things to be aware of\n\nmaster vs main\nIf you think you did something wrong, stop and ask for help\nThere are guardrails in place. But those won’t stop a bulldozer.\nThe hardest part is the initial setup. Then, this should all be rinse-and-repeat.\nThis book is great: Happy Git with R\n\nSee Chapter 6 if you have install problems.\nSee Chapter 10 for credential caching (avoid typing a password all the time)\nSee Chapter 13 if RStudio can’t find git"
  },
  {
    "objectID": "slides/git.html#the-maindevelopbranch-workflow",
    "href": "slides/git.html#the-maindevelopbranch-workflow",
    "title": "Version control",
    "section": "The main/develop/branch workflow",
    "text": "The main/develop/branch workflow\n\nWhen working on your own\n\nDon’t NEED branches (but you should use them, really)\nI make a branch if I want to try a modification without breaking what I have.\n\nWhen working on a large team with production grade software\n\nmain is protected, released version of software (maybe renamed to release)\ndevelop contains things not yet on main, but thoroughly tested\nOn a schedule (once a week, once a month) develop gets merged to main\nYou work on a feature branch off develop to build your new feature\nYou do a PR against develop. Supervisors review your contributions\n\n\n\nI and many DS/CS/Stat faculty use this workflow with my lab."
  },
  {
    "objectID": "slides/git.html#protection",
    "href": "slides/git.html#protection",
    "title": "Version control",
    "section": "Protection",
    "text": "Protection\n\nTypical for your PR to trigger tests to make sure you don’t break things\nTypical for team members or supervisors to review your PR for compliance\n\n\n\n\n\n\n\nTip\n\n\nI suggest you adopt the “production” version for your Team Assignments"
  },
  {
    "objectID": "slides/git.html#guardrails",
    "href": "slides/git.html#guardrails",
    "title": "Version control",
    "section": "Guardrails",
    "text": "Guardrails\n\nThe .github directory contains interactions with GitHub\n\nActions: On push / PR / other GitHub does something on their server (builds a website, runs tests on code)\nPR templates: Little admonitions when you open a PR\nBranch protection: prevent you from doing stuff\n\nIn this course, I protect main so that you can’t push there\n\n\n\n\n\n\n\nWarning\n\n\nIf you try to push to main, it will give an error like\nremote: error: GH006: Protected branch update failed for refs/heads/main.\nThe fix is: make a new branch, then push that."
  },
  {
    "objectID": "slides/git.html#operations-in-rstudio",
    "href": "slides/git.html#operations-in-rstudio",
    "title": "Version control",
    "section": "Operations in Rstudio",
    "text": "Operations in Rstudio\n\n\n\nStage\nCommit\nPush\nPull\nCreate a branch\n\n\n\n\n\n\n\nCovers:\n\n\n\nEverything to do your HW / Project if you’re careful\nPlus most other things you “want to do”\n\n\n\n\n\nCommand line versions (of the same)\ngit add <name/of/file>\n\ngit commit -m \"some useful message\"\n\ngit push\n\ngit pull\n\ngit checkout -b <name/of/branch>"
  },
  {
    "objectID": "slides/git.html#other-useful-stuff-but-command-line-only",
    "href": "slides/git.html#other-useful-stuff-but-command-line-only",
    "title": "Version control",
    "section": "Other useful stuff (but command line only)",
    "text": "Other useful stuff (but command line only)\n\n\nInitializing\ngit config user.name --global \"Daniel J. McDonald\"\ngit config user.email --global \"daniel@stat.ubc.ca\"\ngit config core.editor --global nano \n# or emacs or ... (default is vim)\nStaging\ngit add name/of/file # stage 1 file\ngit add . # stage all\nCommitting\n# stage/commit simultaneously\ngit commit -am \"message\" \n\n# open editor to write long commit message\ngit commit \nPushing\n# If branchname doesn't exist\n# on remote, create it and push\ngit push -u origin branchname\n\nBranching\n# switch to branchname, error if uncommitted changes\ngit checkout branchname \n# switch to a previous commit\ngit checkout aec356\n\n# create a new branch\ngit branch newbranchname\n# create a new branch and check it out\ngit checkout -b newbranchname\n\n# merge changes in branch2 onto branch1\ngit checkout branch1\ngit merge branch2\n\n# grab a file from branch2 and put it on current\ngit checkout branch2 -- name/of/file\n\ngit branch -v # list all branches\nCheck the status\ngit status\ngit remote -v # list remotes\ngit log # show recent commits, msgs"
  },
  {
    "objectID": "slides/git.html#conflicts",
    "href": "slides/git.html#conflicts",
    "title": "Version control",
    "section": "Conflicts",
    "text": "Conflicts\n\nSometimes you merge things and “conflicts” happen.\nMeaning that changes on one branch would overwrite changes on a different branch.\n\n\n\n\nThey look like this:\n\nHere are lines that are either unchanged from\nthe common ancestor, or cleanly resolved \nbecause only one side changed.\n\nBut below we have some troubles\n<<<<<<< yours:sample.txt\nConflict resolution is hard;\nlet's go shopping.\n=======\nGit makes conflict resolution easy.\n>>>>>>> theirs:sample.txt\n\nAnd here is another line that is cleanly \nresolved or unmodified.\n\nYou get to decide, do you want to keep\n\nYour changes (above ======)\nTheir changes (below ======)\nBoth.\nNeither.\n\nBut always delete the <<<<<, ======, and >>>>> lines.\nOnce you’re satisfied, committing resolves the conflict."
  },
  {
    "objectID": "slides/git.html#some-other-pointers",
    "href": "slides/git.html#some-other-pointers",
    "title": "Version control",
    "section": "Some other pointers",
    "text": "Some other pointers\n\nCommits have long names: 32b252c854c45d2f8dfda1076078eae8d5d7c81f\n\nIf you want to use it, you need “enough to be unique”: 32b25\n\nOnline help uses directed graphs in ways different from statistics:\n\nIn stats, arrows point from cause to effect, forward in time\nIn git docs, it’s reversed, they point to the thing on which they depend\n\n\nCheat sheet\nhttps://training.github.com/downloads/github-git-cheat-sheet.pdf"
  },
  {
    "objectID": "slides/git.html#how-to-undo-in-3-scenarios",
    "href": "slides/git.html#how-to-undo-in-3-scenarios",
    "title": "Version control",
    "section": "How to undo in 3 scenarios",
    "text": "How to undo in 3 scenarios\n\nSuppose we’re concerned about a file named README.md\nOften, git status will give some of these as suggestions\n\n\n\n1. Saved but not staged\n\nIn RStudio, select the file and click   then select  Revert…\n\n# grab the previously committed version\ngit checkout -- README.md \n2. Staged but not committed\n\nIn RStudio, uncheck the box by the file, then use the method above.\n\n# unstage\ngit reset HEAD README.md\ngit checkout -- README.md\n\n3. Committed\n\nNot easy to do in RStudio…\n\n# check the log to see where you made the chg, \ngit log\n# go one step before that (eg to 32b252)\n# and grab that earlier version\ngit checkout 32b252 -- README.md\n\n# alternatively\n# if it happens to also be on another branch\ngit checkout otherbranch -- README.md"
  },
  {
    "objectID": "slides/git.html#recovering-from-things",
    "href": "slides/git.html#recovering-from-things",
    "title": "Version control",
    "section": "Recovering from things",
    "text": "Recovering from things\n\nAccidentally did work on main, Tried to Push but got refused\n\n# make a new branch with everything, but stay on main\ngit branch newbranch\n# find out where to go to\ngit log\n# undo everything after ace2193\ngit reset --hard ace2193\ngit checkout newbranch\n\nMade a branch, did lots of work, realized it’s trash, and you want to burn it\n\ngit checkout main\ngit branch -d badbranch\n\nAnything more complicated, either post to Slack or LMGTFY"
  },
  {
    "objectID": "slides/organization.html#i-urge-you-to-consult",
    "href": "slides/organization.html#i-urge-you-to-consult",
    "title": "Organization",
    "section": "I urge you to consult:",
    "text": "I urge you to consult:\nKarl Broman’s Notes"
  },
  {
    "objectID": "slides/organization.html#organizing-your-stuff",
    "href": "slides/organization.html#organizing-your-stuff",
    "title": "Organization",
    "section": "Organizing your stuff",
    "text": "Organizing your stuff\n\n\n├── Advising\n│   ├── arash\n│   ├── gian-carlo\n├── CV\n├── Computing\n│   ├── FKF\n│   ├── batchtools.slurm.tmpl\n│   ├── computecanada_notes.md\n│   ├── ghclass\n│   └── spatio-temporal-exp-fam\n├── Grants\n│   ├── B&E JSM 2010\n│   ├── CANSSI RRP 2020\n│   ├── NSERC 2020\n├── LettersofRec\n├── Manuscripts\n│   ├── Old\n├── Referee reports\n├── Talks\n│   ├── JobTalk2020\n│   ├── ubc-stat-covid-talk\n│   └── utoronto-grad-advice\n├── Teaching\n│   ├── stat-406\n│   ├── stat-550\n│   ├── zzzz CMU TA\n│   └── zzzz booth\n└── Website\n\nInside a project\n.\n├── README.md\n├── Summary of Goals.rtf\n├── cluster_output\n├── code\n├── data\n├── dsges-github.Rproj\n├── manuscript\n└── waldman-triage\n\n\n\nSeparate raw / processed data\nInclude a README\nIdeally have a MAKEFILE\nUnder version control, shared with collaborator"
  },
  {
    "objectID": "slides/organization.html#basic-principles",
    "href": "slides/organization.html#basic-principles",
    "title": "Organization",
    "section": "Basic principles",
    "text": "Basic principles\n\nDevelop your own system\nPut everything in a common directory\nBe consistent – directory structure; names\nSeparate raw from processed data\nSeparate code from data\nIt should be obvious what code created what files, and what the dependencies are.\nNo hand-editing of data files\nDon’t use spaces in file names\nUse relative paths, not absolute paths\n\n../blah not ~/blah or /users/dajmcdon/blah\nThe {here} package in R is great for this"
  },
  {
    "objectID": "slides/organization.html#problem-coordinating-with-collaborators",
    "href": "slides/organization.html#problem-coordinating-with-collaborators",
    "title": "Organization",
    "section": "Problem: Coordinating with collaborators",
    "text": "Problem: Coordinating with collaborators\n\nWhere to put data that multiple people will work with?\nWhere to put intermediate/processed data?\nWhere to indicate the code that created those processed data files?\nHow to divvy up tasks and know who did what?\nNeed to agree on directory structure and file naming conventions\n\nGitHub is (I think) the ideal solution, but not always feasible."
  },
  {
    "objectID": "slides/organization.html#problem-collaborators-who-dont-use-github",
    "href": "slides/organization.html#problem-collaborators-who-dont-use-github",
    "title": "Organization",
    "section": "Problem: Collaborators who don’t use GitHub",
    "text": "Problem: Collaborators who don’t use GitHub\n\nUse GitHub yourself\nCopy files to/from some shared space\n\nIdeally, in an automated way (Dropbox, S3 Bucket)\nAvoid Word at all costs. Google Docs if needed.\nWord and Git do not mix\nLast resort: Word file in Dropbox. Everything else nicely organized on your end. Rmd file with similar structure to Manuscript that does the analysis.\n\nCommit their changes.\n\n\nOverleaf has Git built in (paid tier). I don’t like Overleaf. Costs money, the viewer is crap and so is the editor. I suggest you avoid it."
  },
  {
    "objectID": "slides/organization.html#using-rmarkdownquarto-for-most-things",
    "href": "slides/organization.html#using-rmarkdownquarto-for-most-things",
    "title": "Organization",
    "section": "Using Rmarkdown/Quarto for most things",
    "text": "Using Rmarkdown/Quarto for most things\nYour goal is to Avoid at all costs:\n\n“How did I create this plot?”\n“Why did I decide to omit those six samples?”\n“Where (on the web) did I find these data?”\n“What was that interesting gene/feature/predictor?”\n\n\nReally useful resource:\n\nEmily Reiderer RmdDD\nTalk Slides"
  },
  {
    "objectID": "slides/organization.html#the-basics",
    "href": "slides/organization.html#the-basics",
    "title": "Organization",
    "section": "The basics",
    "text": "The basics\n1.\nI do all class documents in Rmarkdown/Quarto. Notes, slides, etc. Organized in Github repos:\n├── github-org\n│  ├── admm-cd\n│  ├── class-management\n│  ├── convexity-exercises\n...\n│  └── useful-materials\n├── instructor-private\n│  ├── README.md\n│  └── brief-summary\n└── student-public\n    ├── README.md\n    ├── docs\n    └── lecture-slides"
  },
  {
    "objectID": "slides/organization.html#the-basics-1",
    "href": "slides/organization.html#the-basics-1",
    "title": "Organization",
    "section": "The basics",
    "text": "The basics\n2.\nWhen working out new code for a project, I use a combination of R package (as I get close to completion) and Rmarkdown.\n\nMany stat journals require reproducible, documented code. An R package is great for this.\nRmarkdown alone lets me document as I go.\n\n3.\nMy students are required to give me reports in Rmarkdown or Google docs (for practice)."
  },
  {
    "objectID": "slides/organization.html#for-professional-presentations",
    "href": "slides/organization.html#for-professional-presentations",
    "title": "Organization",
    "section": "For professional presentations",
    "text": "For professional presentations\nI use Rmarkdown + Beamer: Now I use Rmd + Xaringan (see dajmcdon/talk-template)\n---\ntitle: \"Statistical approaches to epidemic forecasting\"\nauthor: \"Daniel J. McDonald\"\ndate: \"10 February 2023\"\noutput:\n  xaringan::moon_reader:\n    css: [src/xaringan-themer.css, src/slides-style.css]\n    nature:\n      beforeInit: [\"src/macros.js\", \"https://platform.twitter.com/widgets.js\"]\n      highlightStyle: github\n      highlightLines: true\n      ratio: 16:9\n      slideNumber: true\n      countIncrementalSlides: true\n    seal: false\n---"
  },
  {
    "objectID": "slides/organization.html#the-old-presentation-when-i-was-in-grad-school",
    "href": "slides/organization.html#the-old-presentation-when-i-was-in-grad-school",
    "title": "Organization",
    "section": "The old presentation (when I was in Grad School)",
    "text": "The old presentation (when I was in Grad School)\n\nWrite lots of LaTeX, R code in separate files\nNeed a figure. Run R code, get figure, save as .pdf.\nRecompile LaTeX. Axes are unreadable. Back to R, rerun R code, …\nRecompile LaTeX. Can’t distinguish lines. Back to R, rerun R code, …\nEtc, etc.\n\nNow:\nCode and Text live in one file. I just recompile."
  },
  {
    "objectID": "slides/organization.html#the-old-manuscript",
    "href": "slides/organization.html#the-old-manuscript",
    "title": "Organization",
    "section": "The old manuscript",
    "text": "The old manuscript\nSimilar to the old presentation.\nNow:\n\nR package with documented code, available on GitHub.\n\nOne script to run the analysis, one to gather the results.\n\nOne .Rmd file to take in the results, do preprocessing, generate all figures.\n\nLaTeX file on Journal style.\n\nThe optimal\nSame as above but with a MAKEFILE to automatically run parts of 1–4 as needed"
  },
  {
    "objectID": "slides/presentations.html#structure",
    "href": "slides/presentations.html#structure",
    "title": "Giving presentations",
    "section": "Structure",
    "text": "Structure\n\nStrategy (applies to papers too)\nDos and don’ts\nPersonal preferences"
  },
  {
    "objectID": "slides/presentations.html#genre",
    "href": "slides/presentations.html#genre",
    "title": "Giving presentations",
    "section": "Genre",
    "text": "Genre\nTalks take many forms (like papers)\n\nDepartment seminar\nShort conference presentation\nClass lecture\n...\n\nCalibrate your talk to the Genre and the Audience\n\n\nA job talk takes much more work than a class presentation\nFor context, after much practice, it takes me about 1 hour per minute of presentation length, depending on the amount of polish.\nMy course lectures take about 4x the target duration.\nGeneral ideas are the same for all styles."
  },
  {
    "objectID": "slides/presentations.html#audience",
    "href": "slides/presentations.html#audience",
    "title": "Giving presentations",
    "section": "Audience",
    "text": "Audience\n\nThink about who you are talking to\n\nStatisticians?\nStudents?\nPotential employer?\nPeople with PhD’s but in other disciplines?\nYour grandma?\n\nRegardless of the audience, I think of dividing the talk roughly in 3rds."
  },
  {
    "objectID": "slides/presentations.html#content",
    "href": "slides/presentations.html#content",
    "title": "Giving presentations",
    "section": "Content",
    "text": "Content\nEach part is a little mini-talk\n\nStarts with the general idea\nDevelops a few details. Strategy: problem/solution or question/answer\nEnds with a takeaway\n\nBut these parts are recalibrated to the audience.\n\nYour Grandma doesn’t want to see math.\nYour employer might, but doesn’t want to hear about \\(\\sigma\\)-fields.\nStatisticians don’t want to see proofs (but might want a sketch).\n..."
  },
  {
    "objectID": "slides/presentations.html#story-structure",
    "href": "slides/presentations.html#story-structure",
    "title": "Giving presentations",
    "section": "Story structure",
    "text": "Story structure\n\n\n\n\n\n\nWhat I often see…\n\n\nOnce upon a time, a young MSc student went into the woods of theory and found some trees. First they looked at one tree, it was oak. Then the looked at the next tree, it was maple. Then they wondered if trees could talk. After three months of wandering, they saw a house…\n\n\n\n\n\n\n\n\n\n\nThe attention grabber\n\n\nAxe-wielding woodsman saves student from wolf attack!\n\n\n\n\n\n\n(Enough details to give the headline.)\nHeadline result.\nHow do we know the result is real. What are the details of computation, inference, methodology.\nDemonstration with empirics."
  },
  {
    "objectID": "slides/presentations.html#you-should-consider",
    "href": "slides/presentations.html#you-should-consider",
    "title": "Giving presentations",
    "section": "You should consider…",
    "text": "You should consider…\n\nAttention span diminishes quickly.\nWhat are the 3-5 takeaways?\nHit your main result at the beginning: this is what I can do that I couldn’t before."
  },
  {
    "objectID": "slides/presentations.html#the-ideal-map",
    "href": "slides/presentations.html#the-ideal-map",
    "title": "Giving presentations",
    "section": "The ideal map",
    "text": "The ideal map\nMap out what you’ve done.\n\nWhat did you find?\nWhat are the implications? Why does audience care?\nHow do we do it?\n\n\nAvoid wandering in the wilderness:\n\nFirst we did this;\nBut that didn’t work, so we tried …\nBut then we added …\nFinally we got to the beach …\nAnd the water was nice …\n\nYou left your audience somewhere in the wilderness."
  },
  {
    "objectID": "slides/presentations.html#words",
    "href": "slides/presentations.html#words",
    "title": "Giving presentations",
    "section": "Words",
    "text": "Words\n\n\nToo many words on a slide is bad\n\nBullet points\nToo densely concentrated are bad\nAre bad\nAre hard to focus on\n\n\nEmpty space is your friend\n\nLorem markdownum et moras et ponendi odores, neu magna per! Tyria meo iungitur videt, frigore terras rogis Anienis poteram, dant. His vallem arma corpore vident nunc nivibus avus, dea. Spatium luce certa cupiunt, lina. Amabam opem, Iovis fecundaque et parum.\nConplecti videndo altum et hunc Iovi fronte maris, cur Aiax, iam fata morsibus, et. Aede virum annis audit modo: meus ramis videri: nec quod insidiisque Aonio tenuem, AI. Trames Iason: nocent hortatus lacteus praebita paternos petit, Paridis aptus prius ut origo furiisque. Mercibus sis nullo aliudve Amathunta sufficit ululatibus, praevalidusque segnis et Dryopen."
  },
  {
    "objectID": "slides/presentations.html#images",
    "href": "slides/presentations.html#images",
    "title": "Giving presentations",
    "section": "Images",
    "text": "Images\n\n\nPictures are good\n\nFlow charts are good.\n\nCareful use of colour is good.\n\nSize is good.\n\ntoo much variation is distracting\n\n\n\n\n\nHow long did you stare at the cat?"
  },
  {
    "objectID": "slides/presentations.html#graphics",
    "href": "slides/presentations.html#graphics",
    "title": "Giving presentations",
    "section": "Graphics",
    "text": "Graphics\n\n\n\n\n\n\n\n\nImportant\n\n\nDefaults are almost always terrible."
  },
  {
    "objectID": "slides/presentations.html#issues-with-the-preceding",
    "href": "slides/presentations.html#issues-with-the-preceding",
    "title": "Giving presentations",
    "section": "Issues with the preceding",
    "text": "Issues with the preceding\n\nColours are awful\nGrey background is distracting\nText size is too small\nLegend position on the side is strange?\nNumbers on the y-axis are nonesense\nWith barchart, y-axis should start at 0.\n.png vs .svg"
  },
  {
    "objectID": "slides/presentations.html#graphics-1",
    "href": "slides/presentations.html#graphics-1",
    "title": "Giving presentations",
    "section": "Graphics",
    "text": "Graphics"
  },
  {
    "objectID": "slides/presentations.html#again",
    "href": "slides/presentations.html#again",
    "title": "Giving presentations",
    "section": "Again",
    "text": "Again\n\n\n\n\n\n\n\nTip\n\n\nI like this, but ~10% of men are colour blind (including some faculty in this department)."
  },
  {
    "objectID": "slides/presentations.html#simulation",
    "href": "slides/presentations.html#simulation",
    "title": "Giving presentations",
    "section": "Simulation",
    "text": "Simulation"
  },
  {
    "objectID": "slides/presentations.html#jargon",
    "href": "slides/presentations.html#jargon",
    "title": "Giving presentations",
    "section": "Jargon",
    "text": "Jargon\n\nBe wary of acronyms (MLE, BLUP, RKHS)\nAgain, think of your audience. MLE is fine for any statistician.\nOthers need definitions in words and written on the slide\nSame for math notation \\(\\bar{X},\\ \\mu,\\ \\sigma,\\ \\mathbf{UDV}^\\top\\)\nAnd for applied work e.g. SNP"
  },
  {
    "objectID": "slides/presentations.html#things-i-hate",
    "href": "slides/presentations.html#things-i-hate",
    "title": "Giving presentations",
    "section": "Things I hate",
    "text": "Things I hate\n Saying “I’m not going to talk about …”  “I’m happy to discuss … later if you’d like”.\n Wiggling your laser pointer at every word. Highlight important things with pretty colours. Use pointer sparingly.\n Playing with your collar, your pockets, your water bottle…\n Staring at your slides …\n Displaying the total number of slides as in 6/85 in the lower right hand corner …\n Running over time. Skipping 6 slides to desperately make the time limit.\n Using the default themes:"
  },
  {
    "objectID": "slides/presentations.html#never-use-tables-of-numbers",
    "href": "slides/presentations.html#never-use-tables-of-numbers",
    "title": "Giving presentations",
    "section": "Never use tables of numbers",
    "text": "Never use tables of numbers\n\nEconomists do this all the time for inexplicable reasons\nI rarely put these in papers either\nIf I’m not going to talk about it, it doesn’t go on the slide\nThere’s no way I’m going to read off the number, certainly not to 4 decimal places\nUse a graph\n\n\n\n\nA graph with 3 dots should be a table of 3 numbers.\nBut why do you have only 3 numbers?\nAny table can be a better graph.\n\n\n\n\n\n\n\nAsk yourself:\n\n\nIs this the best way to display the data? Have I summarized too much?"
  },
  {
    "objectID": "slides/presentations.html#things-you-should-do",
    "href": "slides/presentations.html#things-you-should-do",
    "title": "Giving presentations",
    "section": "Things you should do",
    "text": "Things you should do\n Number your slides\n Have lots of prepared backup slides (details, answers to potential questions, further analysis)\n Practice a lot. Practice in front of others. Practice the beginning more than the rest.\n BE EXCITED. You worked hard on this. All results are cool. Play them up. You did something good and you want to tell everyone about how awesome you are. Own it.\n Take credit. Say “I showed this” not “It can be shown”."
  },
  {
    "objectID": "slides/presentations.html#things-that-are-debatable",
    "href": "slides/presentations.html#things-that-are-debatable",
    "title": "Giving presentations",
    "section": "Things that are debatable",
    "text": "Things that are debatable\n\nMath talks tend to be “chalkboard”\nCS talks tend to be “sales pitch”\nStats is in the middle.\nI lean toward details with elements of salesmanship\nIf I hear your talk, I want to be able to “do” what you created. This is hard without some math.\nThis also colours my decisions about software.\n\n\n\n\n\n\n\n\nNote\n\n\nJeff Bezos banned Powerpoint from Amazon presentations"
  },
  {
    "objectID": "slides/presentations.html#closing-suggestions",
    "href": "slides/presentations.html#closing-suggestions",
    "title": "Giving presentations",
    "section": "Closing suggestions",
    "text": "Closing suggestions\nSlow down\n\nGet a bottle of water before the talk. Drink it to pause on (pre-planned) key slides.\nThis will help you relax. It will also give the audience a few seconds to get the hard stuff into their head.\n\nCut back\n\nMost of your slides probably have too many words.\nAnd too much “filler” –> Kill the filler\n\nTry to move\n\nIt’s good to move physically, engage the audience\nTry to make eye contact with the whole room\nRecord yourself once to see if you do anything extraneous\n\nHave fun."
  },
  {
    "objectID": "slides/syllabus.html#about-me",
    "href": "slides/syllabus.html#about-me",
    "title": "Introduction and Second half pivot",
    "section": "About me",
    "text": "About me\n\n\n\nDaniel J. McDonald\ndaniel@stat.ubc.ca\nhttp://dajmcdon.github.io\nAssociate Professor, Department of Statistics\n\n\n\nMoved to UBC in mid-March 2020, 2 days before the border closed\nPreviously a Stats Prof at Indiana University for 8 years"
  },
  {
    "objectID": "slides/syllabus.html#no-more-canvas",
    "href": "slides/syllabus.html#no-more-canvas",
    "title": "Introduction and Second half pivot",
    "section": "No More Canvas!!",
    "text": "No More Canvas!!\nSee the website:\nhttps://ubc-stat.github.io/stat-550/\n\n\n\nYou’ll find\n\nannouncements\nschedule\nlecture slides / notes\n\n(Grades still on Canvas)"
  },
  {
    "objectID": "slides/syllabus.html#course-communication",
    "href": "slides/syllabus.html#course-communication",
    "title": "Introduction and Second half pivot",
    "section": "Course communication",
    "text": "Course communication\n\n\nWebsite:\nhttps://ubc-stat.github.io/stat-550\n\nHosted on GitHub.\nLinks to slides and all materials\nSyllabus is there. Be sure to read it. (same idea as before)\n\nSlack:\n\nThis is our discussion board.\nNote that this data is hosted on servers outside of Canada. You may wish to use a pseudonym to protect your privacy.\nWe’ll use a Channel in the UBC-Stat Workspace\n\n\nGithub organization\n\nLinked from the website.\nThis is where you complete/submit assignments/projects/in-class-work\nThis is also hosted on US servers https://github.com/Stat550-2022"
  },
  {
    "objectID": "slides/syllabus.html#why-these",
    "href": "slides/syllabus.html#why-these",
    "title": "Introduction and Second half pivot",
    "section": "Why these?",
    "text": "Why these?\n\nYes, some data is hosted on servers in the US.\nBut in the real world, no one uses Canvas/Piazza, so why not learn things they do use?\nCanvas is dumb and hard to organize.\nGitHub is free and actually useful.\nMuch easier to communicate, “grade” or comment on your work\nMuch more DS friendly\nNote that MDS uses both of these, the department uses both, etc.\nMore on all this later.\n\n\nSlack help from MDS — features and rules"
  },
  {
    "objectID": "slides/syllabus.html#what-are-the-goals-of-stat-550",
    "href": "slides/syllabus.html#what-are-the-goals-of-stat-550",
    "title": "Introduction and Second half pivot",
    "section": "What are the goals of Stat 550?",
    "text": "What are the goals of Stat 550?\n\n1. Prepare you to do the consulting practicum (Stat 551)\n\n2. You’re a captive audience, so I can teach you some skills you’ll need for\n\nMSc Thesis/Project or PhD research\nEmployment in Data Science / Statistics.\nThese are often things that will help with the first as well"
  },
  {
    "objectID": "slides/syllabus.html#prepare-you-for-the-consulting-practicum-stat-551",
    "href": "slides/syllabus.html#prepare-you-for-the-consulting-practicum-stat-551",
    "title": "Introduction and Second half pivot",
    "section": "1. Prepare you for the consulting practicum (Stat 551)",
    "text": "1. Prepare you for the consulting practicum (Stat 551)\n\nunderstand how the data was collected\nimplications of the collection process for analysis\norganize data for analysis\ndetermine appropriate methods for analysis that answer’s the client’s questions\ninterpret the results\npresent and communicate the results\n\n\n\nIn most courses you get nice clean data. Getting to “nice clean data” is non-trivial\nIn most courses things are “IID”, negligible missingness\nUsually, the question is formed in statistical langauge, here, you are responsible for “translating”\nInterpretation has to be “translated back”\nPresentation skills — important everywhere"
  },
  {
    "objectID": "slides/syllabus.html#some-skills-youll-need",
    "href": "slides/syllabus.html#some-skills-youll-need",
    "title": "Introduction and Second half pivot",
    "section": "2. Some skills you’ll need",
    "text": "2. Some skills you’ll need\n\nVersion control\nReproducible reports\nWriting experience: genre is important\nPresentation skills\nBetter coding practice\nDocumentation"
  },
  {
    "objectID": "slides/syllabus.html#computing",
    "href": "slides/syllabus.html#computing",
    "title": "Introduction and Second half pivot",
    "section": "Computing",
    "text": "Computing\n\nAll work done in R/RMarkdown.\nNo you can’t use Python. Or Stata or SPSS.\nNo you can’t use Jupyter Notebooks.\nAll materials on Github.\nYou will learn to use Git/GitHub/RStudio/Rmarkdown.\nSlack for discussion/communication"
  },
  {
    "objectID": "slides/syllabus.html#getting-setup",
    "href": "slides/syllabus.html#getting-setup",
    "title": "Introduction and Second half pivot",
    "section": "Getting setup",
    "text": "Getting setup\n\nAdd to Slack Channel: https://ubc-stat.slack.com/archives/C04QUDNJG9X\nGithub account: https://github.com/\nAdd to the Github Org — tell me your account\nRStudio synchronization"
  },
  {
    "objectID": "slides/time-series.html#the-general-linear-process",
    "href": "slides/time-series.html#the-general-linear-process",
    "title": "Time series, a whirlwind",
    "section": "The general linear process",
    "text": "The general linear process\n\nImagine that there is a noise process\n\n\\[\\epsilon_j \\sim \\textrm{N}(0, 1),\\ \\textrm{i.i.d.}\\]\n\nAt time \\(i\\), we observe the sum of all past noise\n\n\\[y_i = \\sum_{j=-\\infty}^0 a_{i+j} \\epsilon_j\\]\n\nWithout some conditions on \\(\\{a_k\\}_{k=-\\infty}^0\\) this process will “run away”\nThe result is “non-stationary” and difficult to analyze.\nStationary means (roughly) that the marginal distribution of \\(y_i\\) does not change with \\(i\\)."
  },
  {
    "objectID": "slides/time-series.html#chasing-stationarity",
    "href": "slides/time-series.html#chasing-stationarity",
    "title": "Time series, a whirlwind",
    "section": "Chasing stationarity",
    "text": "Chasing stationarity\n\nn <- 1000\nset.seed(12345)\nnseq <- 5\ngenerate_ar <- function(id, n, b) {\n  y <- double(n)\n  y[1] <- rnorm(1)\n  for (i in 2:n) y[i] <- b * y[i - 1] + rnorm(1)\n  tibble(time = 1:n, y = y, id = id)\n}\nstationary <- map_dfr(1:nseq, ~ generate_ar(.x, n = n, b = .99))\nnon_stationary <- map_dfr(1:nseq, ~ generate_ar(.x, n = n, b = 1.01))"
  },
  {
    "objectID": "slides/time-series.html#uses-of-stationarity",
    "href": "slides/time-series.html#uses-of-stationarity",
    "title": "Time series, a whirlwind",
    "section": "Uses of stationarity",
    "text": "Uses of stationarity\n\nLots of types (weak, strong, in-mean, wide-sense,…)\nnot required for modelling / forecasting\nBut assuming stationarity gives some important guarantees\nUsually work with stationary processes"
  },
  {
    "objectID": "slides/time-series.html#standard-models",
    "href": "slides/time-series.html#standard-models",
    "title": "Time series, a whirlwind",
    "section": "Standard models",
    "text": "Standard models\nAR(p)\nSuppose \\(\\epsilon_i\\) are i.i.d. N(0, 1) (distn is convenient, but not required)\n\\[y_i = \\mu + a_1 y_{i-1} + \\cdots + a_p y_{i-p} + \\epsilon_i\\]\n\nThis is a special case of the general linear process\nYou can recursively substitute this defn into itself to get that equation\n\nEasy to estimate the a’s given a realization.\n\ny <- arima.sim(list(ar = c(.7, -.1)), n = 1000)\nY <- y[3:1000]\nX <- cbind(lag1 = y[2:999], lag2 = y[1:998])\nsummary(lm(Y ~ X + 0))\n\n\nCall:\nlm(formula = Y ~ X + 0)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6164 -0.6638  0.0271  0.6456  3.8367 \n\nCoefficients:\n      Estimate Std. Error t value Pr(>|t|)    \nXlag1  0.66931    0.03167  21.134   <2e-16 ***\nXlag2 -0.04856    0.03167  -1.533    0.126    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9899 on 996 degrees of freedom\nMultiple R-squared:  0.4085,    Adjusted R-squared:  0.4073 \nF-statistic:   344 on 2 and 996 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "slides/time-series.html#arp-1",
    "href": "slides/time-series.html#arp-1",
    "title": "Time series, a whirlwind",
    "section": "AR(p)",
    "text": "AR(p)\n\nThe estimate isn’t that accurate because the residuals (not the \\(\\epsilon\\)’s) are correlated.\n(Usually, you get 1/n convergence, here you don’t.)\nAlso, this isn’t the MLE. The likelihood includes \\(p(y_1)\\), \\(p(y_2 | y_1)\\) which lm() ignored.\nThe Std. Errors are unjustified.\nBut that was easy to do.\nThe correct way is\n\n\n\n\nCall:\narima(x = y, order = c(2, 0, 0), include.mean = FALSE)\n\nCoefficients:\n         ar1      ar2\n      0.6686  -0.0485\ns.e.  0.0316   0.0316\n\nsigma^2 estimated as 0.9765:  log likelihood = -1407.34,  aic = 2820.67\n\n\n\nThe resulting estimates and SEs are identical, AFAICS."
  },
  {
    "objectID": "slides/time-series.html#maq",
    "href": "slides/time-series.html#maq",
    "title": "Time series, a whirlwind",
    "section": "MA(q)",
    "text": "MA(q)\nStart with the general linear process, but truncate the infinite sum.\n\\[y_i = \\sum_{j=-q}^0 a_{i+j} \\epsilon_j\\]\n\nThis is termed a “moving average” process.\nthough \\(a_0 + \\cdots a_{-q}\\) don’t sum to 1.\nCan’t write this easily as a lm()\n\n\ny <- arima.sim(list(ma = c(.9, .6, .1)), n = 1000)\narima(y, c(0, 0, 3), include.mean = FALSE)\n\n\nCall:\narima(x = y, order = c(0, 0, 3), include.mean = FALSE)\n\nCoefficients:\n         ma1     ma2     ma3\n      0.9092  0.6069  0.1198\ns.e.  0.0313  0.0380  0.0311\n\nsigma^2 estimated as 0.8763:  log likelihood = -1353.41,  aic = 2714.82"
  },
  {
    "objectID": "slides/time-series.html#maq-as-an-ar1-hidden-process",
    "href": "slides/time-series.html#maq-as-an-ar1-hidden-process",
    "title": "Time series, a whirlwind",
    "section": "MA(q) as an AR(1) hidden process",
    "text": "MA(q) as an AR(1) hidden process\nLet \\(X_j = [\\epsilon_{j-1},\\ \\ldots,\\  \\epsilon_{j-q}]\\) and write\n\\[\n\\begin{aligned}\nX_i &= \\begin{bmatrix} a_{i-1} & a_{i-2} & \\cdots & a_{i-q}\\\\ 1 & 0 & \\cdots & 0\\\\ & & \\ddots \\\\ 0 & 0 & \\cdots & 1\\end{bmatrix} X_{i-1} +\n\\begin{bmatrix} a_{i}\\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\epsilon_i\\\\\ny_i &= \\begin{bmatrix} 1 & 0 & \\cdots 0 \\end{bmatrix} X_i\n\\end{aligned}\n\\]\n\nNow \\(X\\) is a \\(q\\)-dimensional AR(1) (but we don’t see it)\n\\(y\\) is deterministic conditional on \\(X\\)\nThis is the usual way these are estimated using a State-Space Model\nMany time series models have multiple equivalent representations"
  },
  {
    "objectID": "slides/time-series.html#arima",
    "href": "slides/time-series.html#arima",
    "title": "Time series, a whirlwind",
    "section": "ARIMA",
    "text": "ARIMA\n\nWe’ve been using arima() and arima.sim(), so what is left?\nThe “I” means “integrated”\nIf, for example, we can write \\(z_i = y_i - y_{i-1}\\) and \\(z\\) follows an ARMA(p, q), we say \\(y\\) follows an ARIMA(p, 1, q).\nThe middle term is the degree of differencing"
  },
  {
    "objectID": "slides/time-series.html#other-standard-models",
    "href": "slides/time-series.html#other-standard-models",
    "title": "Time series, a whirlwind",
    "section": "Other standard models",
    "text": "Other standard models\nSuppose we can write\n\\[\ny_i = T_i + S_i + W_i\n\\]\nThis is the “classical” decomposition of \\(y\\) into a Trend + Seasonal + Noise.\nYou can estimate this with a “Basic Structural Time Series Model” using StrucTS().\nA related, though slightly different model is called the STL decomposition, estimated with stl().\nThis is “Seasonal Decomposition of Time Series by Loess”\n(LOESS is “locally estimated scatterplot smoothing” named/proposed independently by Bill Cleveland though originally proposed about 15 years earlier and called the Savitsky-Golay Filter)"
  },
  {
    "objectID": "slides/time-series.html#quick-example",
    "href": "slides/time-series.html#quick-example",
    "title": "Time series, a whirlwind",
    "section": "Quick example",
    "text": "Quick example\n\nsts <- StructTS(AirPassengers)\nbc <- stl(AirPassengers, \"periodic\") # use sin/cos to represent the seasonal\ntibble(time = seq(as.Date(\"1949-01-01\"), as.Date(\"1960-12-31\"), by = \"month\"),\n       AP = AirPassengers, StrucTS = fitted(sts)[,1], STL = rowSums(bc$time.series[,1:2])) %>%\n  pivot_longer(-time) %>%\n  ggplot(aes(time, value, color = name)) +\n  geom_line() +\n  theme_bw(base_size = 24) +\n  scale_color_viridis_d(name = \"\") +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "slides/time-series.html#generic-state-space-model",
    "href": "slides/time-series.html#generic-state-space-model",
    "title": "Time series, a whirlwind",
    "section": "Generic state space model",
    "text": "Generic state space model\n\n\n\n\\[\\begin{aligned} x_k &\\sim p(x_k | x_{k-1}) \\\\ y_k &\\sim p(y_k | x_k)\\end{aligned}\\]\n\n\n\\(x_k\\) is unobserved, dimension \\(n\\)\n\\(y_k\\) is observed, dimension \\(m\\)\n\\(x\\) process is the transition or process equation\n\\(y\\) is the observation or measurement equation\nBoth are probability distributions that can depend on parameters \\(\\theta\\)\nFor now, assume \\(\\theta\\) is KNOWN\nWe can allow the densities to vary with time."
  },
  {
    "objectID": "slides/time-series.html#goals",
    "href": "slides/time-series.html#goals",
    "title": "Time series, a whirlwind",
    "section": "GOAL(s)",
    "text": "GOAL(s)\n\nFiltering: given observations, find \\[p(x_k | y_1,\\ldots y_k)\\]\nSmoothing: given observations, find \\[p(x_k | y_1,\\ldots y_T), \\;\\;\\ k < T\\]\nForecasting: given observations, find \\[p(y_{k+1} | y_1,\\ldots,y_k)\\]"
  },
  {
    "objectID": "slides/time-series.html#using-bayes-rule",
    "href": "slides/time-series.html#using-bayes-rule",
    "title": "Time series, a whirlwind",
    "section": "Using Bayes Rule",
    "text": "Using Bayes Rule\nAssume \\(p(x_0)\\) is known\n\\[\n\\begin{aligned}\np(y_1,\\ldots,y_T\\ |\\ x_1, \\ldots, x_T) &= \\prod_{k=1}^T p(y_k | x_k)\\\\\np(x_0,\\ldots,x_T) &= p(x_0) \\prod_{k=1}^T p(x_k | x_{k-1})\\\\\np(x_0,\\ldots,x_T\\ |\\ y_1,\\ldots,y_T) &= \\frac{p(y_1,\\ldots,y_T\\ |\\ x_1, \\ldots, x_T)p(x_0,\\ldots,x_T)}{p(y_1,\\ldots,y_T)}\\\\ &\\propto p(y_1,\\ldots,y_T\\ |\\ x_1, \\ldots, x_T)p(x_0,\\ldots,x_T)\\end{aligned}\n\\]\nIn principle, if things are nice, you can compute this posterior (thinking of \\(x\\) as unknown parameters)\nBut in practice, computing a big multivariate posterior like this is computationally ill-advised."
  },
  {
    "objectID": "slides/time-series.html#generic-filtering",
    "href": "slides/time-series.html#generic-filtering",
    "title": "Time series, a whirlwind",
    "section": "Generic filtering",
    "text": "Generic filtering\n\nRecursively build up \\(p(x_k | y_1,\\ldots y_k)\\).\nWhy? Because if we’re collecting data in real time, this is all we need to make forecasts for future data.\n\n\\[\\begin{aligned} &p(y_{T+1} | y_1,\\ldots,y_T)\\\\ &= p(y_{T+1} | x_{T+1}, y_1,\\ldots,y_T)\\\\ &= p(y_{T+1} | x_{T+1} )p(x_{T+1} | y_1,\\ldots,y_T)\\\\ &= p(y_{T+1} | x_{T+1} )p(x_{T+1} | x_T) p(x_T | y_1,\\ldots,y_T)\\end{aligned}\\]\n\nCan continue to iterate if I want to predict \\(h\\) steps ahead\n\n\\[\\begin{aligned} &p(y_{T+h} | y_1,\\ldots,y_T)= p(y_{T+h} | x_{T+h} )\\prod_{j=0}^{h-1} p(x_{T+j+1} | x_{T+j}) p(x_T | y_1,\\ldots,y_T)\\end{aligned}\\]"
  },
  {
    "objectID": "slides/time-series.html#the-filtering-recursion",
    "href": "slides/time-series.html#the-filtering-recursion",
    "title": "Time series, a whirlwind",
    "section": "The filtering recursion",
    "text": "The filtering recursion\n\nInitialization. Fix \\(p(x_0)\\).\n\nIterate the following for \\(k=1,\\ldots,T\\):\n\nPredict. \\[p(x_k | y_{k-1}) = \\int p(x_k | x_{k-1}) p(x_{k-1} | y_1,\\ldots, y_{k-1})dx_{k-1}.\\]\nUpdate. \\[p(x_k | y_1,\\ldots,y_k) = \\frac{p(y_k | x_k)p(x_k | y_1,\\ldots,y_{k-1})}{p(y_1,\\ldots,y_k)}\\]\n\nIn general, this is somewhat annoying because these integrals may be challenging to solve.\nBut with some creativity, we can use Monte Carlo for everything."
  },
  {
    "objectID": "slides/time-series.html#what-if-we-make-lots-of-assumptions",
    "href": "slides/time-series.html#what-if-we-make-lots-of-assumptions",
    "title": "Time series, a whirlwind",
    "section": "What if we make lots of assumptions?",
    "text": "What if we make lots of assumptions?\nAssume that \\[\\begin{aligned}p(x_0) &= N(m_0, P_0) \\\\ p_k(x_k\\ |\\ x_{k-1}) &= N(A_{k-1}x_{k-1},\\ Q_{k-1})\\\\ p_k(y_k\\ |\\ x_k) &= N(H_k x_k,\\ R_k)\\end{aligned}.\\]\nThen all the ugly integrals have closed-form representations by properties of conditional Gaussian distributions."
  },
  {
    "objectID": "slides/time-series.html#closed-form-representations",
    "href": "slides/time-series.html#closed-form-representations",
    "title": "Time series, a whirlwind",
    "section": "Closed-form representations",
    "text": "Closed-form representations\n\n\nDistributions:\n\\[\n\\begin{aligned}\np(x_k | y_1,\\ldots,y_{k-1}) &= N(m^{-}_k, P^{-}_k)\\\\\np(x_k | y_1,\\ldots,y_{k}) &= N(m_k, P_k)\\\\\np(y_{k} | y_1,\\ldots,y_{k-1}) &= N(H_k m^-_k, S_k)\\\\\n\\end{aligned}\n\\] Prediction: \\[\n\\begin{aligned}\nm^-_k &= A_{k-1}m_{k-1}\\\\\nP^-_k &= A_{k-1}P_{k-1}A^\\mathsf{T}_{k-1} + Q_{k-1}\n\\end{aligned}\n\\]\n\nUpdate: \\[\n\\begin{aligned}\nv_k &= y_k - H_k m_k^-\\\\\nS_k &= H_k P_k^- H_k^\\mathsf{T} + R_k\\\\\nK_k &= P^-_k H_k^\\mathsf{T} S_k^{-1}\\\\\nm_k &= m^-_k + K_{k}v_{k}\\\\\nP_k &= P^-_k - K_k S_k K_k^\\mathsf{T}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/time-series.html#code-or-it-isnt-real-kalman-filter",
    "href": "slides/time-series.html#code-or-it-isnt-real-kalman-filter",
    "title": "Time series, a whirlwind",
    "section": "Code or it isn’t real (Kalman Filter)",
    "text": "Code or it isn’t real (Kalman Filter)\n\n\n\nkalman <- function(y, m0, P0, A, Q, H, R) {\n  n <- length(y)\n  m <- double(n+1)\n  P <- double(n+1)\n  m[1] <- m0\n  P[1] <- P0\n  for (k in seq(n)) {\n    mm <- A * m[k]\n    Pm <- A * P[k] * A + Q\n    v <- y[k] - H * mm\n    S <- H * Pm * H + R\n    K <- Pm * H / S\n    m[k+1] <- mm + K * v\n    P[k+1] <- Pm - K * S * K\n  }\n  tibble(t = 1:n, m = m[-1], P = P[-1])\n}\n\nset.seed(2022-06-01)\nx <- double(100)\nfor (k in 2:100) x[k] = x[k - 1] + rnorm(1)\ny <- x + rnorm(100, sd = 1)\nkf <- kalman(y, 0, 5, 1, 1, 1, 1)"
  },
  {
    "objectID": "slides/time-series.html#important-notes",
    "href": "slides/time-series.html#important-notes",
    "title": "Time series, a whirlwind",
    "section": "Important notes",
    "text": "Important notes\n\nSo far, we assumed all parameters were known.\nIn reality, we had 6: m0, P0, A, Q, H, R\nI sort of also think of x as “parameters” in the Bayesian sense\nBy that I mean, “latent variables for which we have prior distributions”\nWhat if we want to estimate them?\n\nBayesian way: m0 and P0 are already the parameters of for the prior on x1. Put priors on the other 4.\nFrequentist way: Just maximize the likelihood. Can technically take P0 \\(\\rightarrow\\infty\\) to remove it and m0\n\nThe Likelihood is produced as a by-product of the Kalman Filter.\n\\[-\\ell(\\theta) = \\sum_{k=1}^T \\left(v_k^\\mathsf{T}S_k^{-1}v_k + \\log |S_k| + m \\log 2\\pi\\right)\\]"
  },
  {
    "objectID": "slides/time-series.html#smoothing",
    "href": "slides/time-series.html#smoothing",
    "title": "Time series, a whirlwind",
    "section": "Smoothing",
    "text": "Smoothing\n\nWe also want \\(p(x_k | y_1,\\ldots,y_{T})\\)\nFiltering went “forward” in time. At the end we got, \\(p(x_T | y_1,\\ldots,y_{T})\\). Smoothing starts there and goes “backward”\nFor “everything linear Gaussian”, this is again “easy”\nSet \\(m_T^s = m_T\\), \\(P_T^s = P_T\\).\nFor \\(k = T-1,\\ldots,1\\),\n\n\\[\\begin{aligned}\nG_k &= P_k A_k^\\mathsf{T} [P_{k+1}^-]^{-1}\\\\\nm_k^s &= m_k + G_k(m_{k+1}^s - m_{k+1}^-)\\\\\nP_k^s &= P_k + G_k(P_{k+1}^s - P_{k+1}^-)G_k^\\mathsf{T}\\\\\nx_k | y_1,\\ldots,y_T &= N(m^s_k, P_k^s)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/time-series.html#comparing-the-filter-and-the-smoother",
    "href": "slides/time-series.html#comparing-the-filter-and-the-smoother",
    "title": "Time series, a whirlwind",
    "section": "Comparing the filter and the smoother",
    "text": "Comparing the filter and the smoother\n\nSame data, different code (using a package)\n\n\nlibrary(FKF)\nfilt <- fkf(\n  a0 = 0, P0 = matrix(5), dt = matrix(0), ct = matrix(0), \n  Tt = matrix(1), Zt = matrix(1), HHt = matrix(1), GGt = matrix(1), \n  yt = matrix(y, ncol = length(y)))\nsmo <- fks(filt)"
  },
  {
    "objectID": "slides/time-series.html#what-about-non-linear-andor-non-gaussian",
    "href": "slides/time-series.html#what-about-non-linear-andor-non-gaussian",
    "title": "Time series, a whirlwind",
    "section": "What about non-linear and/or non-Gaussian",
    "text": "What about non-linear and/or non-Gaussian\n\\[\\begin{aligned} x_k &\\sim p(x_k | x_{k-1}) \\\\ y_k &\\sim p(y_k | x_k)\\end{aligned}\\]\nThen we need to solve integrals. This is a pain. We approximate them.\nThese all give approximations to the filtering distribution\n\nExtended Kalman filter - basically do a Taylor approximation, then do Kalman like\nUncented Kalman filter - Approximate integrals with Sigma points\nParticle filter - Sequential Monte Carlo\nBootstrap filter (simple version of SMC)\nLaplace Gaussian filter - Do a Laplace approximation to the distributions"
  },
  {
    "objectID": "slides/time-series.html#the-bootstrap-filter",
    "href": "slides/time-series.html#the-bootstrap-filter",
    "title": "Time series, a whirlwind",
    "section": "The bootstrap filter",
    "text": "The bootstrap filter\n\nNeed to simulate from the transition distribution (rtrans)\nNeed to evaluate the observation distribution (dobs)\n\n\nboot_filter <- \n  function(y, B = 1000, rtrans, dobs, a0 = 0, P0 = 1, perturb = function(x) x) {\n    n <- length(y)\n    filter_est <- matrix(0, n, B)\n    predict_est <- matrix(0, n, B)\n    init <- rnorm(B, a0, P0)\n    filter_est[1, ] = init\n    for (i in seq(n)) {\n      raw_w <- dobs(y[i], filter_est[i, ])\n      w <- raw_w / sum(raw_w)\n      selection <- sample.int(B, replace = TRUE, prob = w)\n      filter_est[i, ] <- perturb(filter_est[i, selection])\n      predict_est[i, ] <- rtrans(filter_est[i, ])\n      if (i < n) filter_est[i + 1, ] <- predict_est[i, ]\n    }\n    list(filt = filter_est, pred = predict_est)\n  }"
  },
  {
    "objectID": "slides/unit-tests.html#i-urge-you-to-consult",
    "href": "slides/unit-tests.html#i-urge-you-to-consult",
    "title": "Unit tests",
    "section": "I urge you to consult:",
    "text": "I urge you to consult:\nCarnegie Mellon’s 36-750 Notes\nThank you Alex and Chris for the heavy lifting."
  },
  {
    "objectID": "slides/unit-tests.html#bugs-happen.-all.-the.-time.",
    "href": "slides/unit-tests.html#bugs-happen.-all.-the.-time.",
    "title": "Unit tests",
    "section": "Bugs happen. All. The. Time.",
    "text": "Bugs happen. All. The. Time.\n\nthe crash of the Mars Climate Orbiter (1998),\na failure of the national telephone network (1990),\na deadly medical device (1985, 2000),\na massive Northeastern blackout (2003),\nthe Heartbleed, Goto Fail, Shellshock exploits (2012–2014),\na 15-year-old fMRI analysis software bug that inflated significance levels (2015),\n\n\nIt is easy to write lots of code.\nBut are we sure it’s doing the right things?\n\n\n\n\n\n\nImportant\n\n\nEffective testing tries to help."
  },
  {
    "objectID": "slides/unit-tests.html#a-common-interactive-workflow",
    "href": "slides/unit-tests.html#a-common-interactive-workflow",
    "title": "Unit tests",
    "section": "A Common (Interactive) Workflow",
    "text": "A Common (Interactive) Workflow\n\nWrite a function.\nTry some reasonable values at the REPL to check that it works.\nIf there are problems, maybe insert some print statements, and modify the function.\nRepeat until things seem fine.\n\n(REPL == Read-Eval-Print-Loop, the console, or Jupyter NB)\n\nThis tends to result in lots of bugs.\nLater on, you forget which values you tried, whether they failed, how you fixed them.\nSo you make a change and maybe or maybe not try some again."
  },
  {
    "objectID": "slides/unit-tests.html#example",
    "href": "slides/unit-tests.html#example",
    "title": "Unit tests",
    "section": "Example:",
    "text": "Example:\n\ntwo_norm <- function(x) sum(x^2)\ngrouped_two_norm <- function(x, gr) as.vector(tapply(x, gr, two_norm))\ngr_two_norm <- function(x, gr) sum(grouped_two_norm(x, gr))\n\n\nThere’s a silly bug in the above code.\n\n\n\nAt one point, I decided that I didn’t want the \\(\\ell_2\\)-norm, I wanted the squared \\(\\ell_2\\) norm.\nBut now the other two functions are wrong.\n\n\n\n\n\n\n\nNote\n\n\nThese functions get used in many other places.\nTo make sure I don’t do something dumb ever again, I write unit tests."
  },
  {
    "objectID": "slides/unit-tests.html#unit-testing",
    "href": "slides/unit-tests.html#unit-testing",
    "title": "Unit tests",
    "section": "Unit Testing",
    "text": "Unit Testing\n\nA unit is a small bit of code (function, class, module, group of classes)\nA test calls the unit with a set of inputs, and checks if we get the expected output.\n\n\ntest_that(\"group norms are correct\", {\n  asparse <- .05\n  gr <- c(1, 1, 1, 1, 2, 2, 3, 3, 3, 3, 3)\n  x <- -5:5\n  expect_equal(two_norm(c(-5 , 5)), sqrt(50)) # this one will fail\n  expect_equal(\n    grouped_two_norm(x, gr),\n    c(two_norm(x[1:4]), two_norm(x[5:6]), two_norm(x[7:11])))\n})\n\nUnit testing consists of writing tests that are\n\nfocused on a small, low-level piece of code (a unit)\ntypically written by the programmer with standard tools\nfast to run (so can be run often, i.e. before every commit)."
  },
  {
    "objectID": "slides/unit-tests.html#unit-testing-benefits",
    "href": "slides/unit-tests.html#unit-testing-benefits",
    "title": "Unit tests",
    "section": "Unit testing benefits",
    "text": "Unit testing benefits\nAmong others:\n\nExposing problems early\nMaking it easy to change (refactor) code without forgetting pieces or breaking things\nSimplifying integration of components\nProviding natural documentation of what the code should do\nDriving the design of new code."
  },
  {
    "objectID": "slides/unit-tests.html#components-of-a-unit-testing-framework",
    "href": "slides/unit-tests.html#components-of-a-unit-testing-framework",
    "title": "Unit tests",
    "section": "Components of a Unit Testing Framework",
    "text": "Components of a Unit Testing Framework\n\n\n\nCollection of Assertions executed in sequence.\nExecuted in a self-contained environment.\nAny assertion fails  Test fails.\n\nEach test focuses on a single component.\nNamed so that you know what it’s doing."
  },
  {
    "objectID": "slides/unit-tests.html#a-test-suite",
    "href": "slides/unit-tests.html#a-test-suite",
    "title": "Unit tests",
    "section": "A test suite",
    "text": "A test suite\n\n\n\nCollection of related tests in a common context.\nPrepares the environment, cleans up after\n(loads some data, connects to database, necessary library,…)\nTest suites are run and the results reported, particularly failures, in a easy to parse and economical style.\nFor example, Python’s {unittest} can report like this\n\n\n$ python test/trees_test.py -v\n\ntest_crime_counts (__main__.DataTreeTest)\nEnsure Ks are consistent with num_points. ... ok\ntest_indices_sorted (__main__.DataTreeTest)\nEnsure all node indices are sorted in increasing order. ... ok\ntest_no_bbox_overlap (__main__.DataTreeTest)\nCheck that child bounding boxes do not overlap. ... ok\ntest_node_counts (__main__.DataTreeTest)\nEnsure that each node's point count is accurate. ... ok\ntest_oversized_leaf (__main__.DataTreeTest)\nDon't recurse infinitely on duplicate points. ... ok\ntest_split_parity (__main__.DataTreeTest)\nCheck that each tree level has the right split axis. ... ok\ntest_trange_contained (__main__.DataTreeTest)\nCheck that child tranges are contained in parent tranges. ... ok\ntest_no_bbox_overlap (__main__.QueryTreeTest)\nCheck that child bounding boxes do not overlap. ... ok\ntest_node_counts (__main__.QueryTreeTest)\nEnsure that each node's point count is accurate. ... ok\ntest_oversized_leaf (__main__.QueryTreeTest)\nDon't recurse infinitely on duplicate points. ... ok\ntest_split_parity (__main__.QueryTreeTest)\nCheck that each tree level has the right split axis. ... ok\ntest_trange_contained (__main__.QueryTreeTest)\nCheck that child tranges are contained in parent tranges. ... ok\n\n---------------------------------------------------------\nRan 12 tests in 23.932s"
  },
  {
    "objectID": "slides/unit-tests.html#r-example",
    "href": "slides/unit-tests.html#r-example",
    "title": "Unit tests",
    "section": "R example",
    "text": "R example\nℹ Loading sparsegl\nℹ Testing sparsegl\n✓ | F W S  OK | Context\n✓ |         7 | model_base                                                           \n✓ |         7 | norms                                                                \n⠏ |         0 | predict                                         \nLoading required package: Matrix\nLoaded glmnet 4.1-3\n✓ |        17 | predict [0.4s]                                                       \n✓ |         5 | risk_estimation [0.4s]                                               \n✓ |         5 | sparsegl_comparisons                                                 \n✓ |        18 | sparsegl_params                                                      \n\n══ Results ══════════════════════════════════════════════════════════════════════════\nDuration: 0.9 s\n\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 59 ]"
  },
  {
    "objectID": "slides/unit-tests.html#what-do-i-test",
    "href": "slides/unit-tests.html#what-do-i-test",
    "title": "Unit tests",
    "section": "What do I test?",
    "text": "What do I test?\n\n\n\nCore Principle:\n\n\nTests should be passed by a correct function, but not by an incorrect function.\n\n\n\nThe tests must apply pressure to know if things break.\n\nseveral specific inputs for which you know the correct answer\n“edge” cases, like a list of size zero or a matrix instead of a vector\nspecial cases that the function must handle, but which you might forget about months from now\nerror cases that should throw an error instead of returning an invalid answer\nprevious bugs you’ve fixed, so those bugs never return."
  },
  {
    "objectID": "slides/unit-tests.html#what-do-i-test-1",
    "href": "slides/unit-tests.html#what-do-i-test-1",
    "title": "Unit tests",
    "section": "What do I test?",
    "text": "What do I test?\nMake sure that incorrect functions won’t pass (or at least, won’t pass them all).\n\nadd <- function(a, b) return(4)\nadd <- function(a, b) return(a * b)\n\ntest_that(\"Addition is commutative\", {\n  expect_equal(add(1, 3), add(3, 1)) # both pass this !!\n  expect_equal(add(2, 5), add(5, 2)) # neither passes this\n})\n\n\n\n\n\n\n\nTip\n\n\n\nCover all branches.\nMake sure there aren’t branches you don’t expect."
  },
  {
    "objectID": "slides/unit-tests.html#test-driven-development",
    "href": "slides/unit-tests.html#test-driven-development",
    "title": "Unit tests",
    "section": "Test-driven development",
    "text": "Test-driven development\nTest Driven Development (TDD) uses a short development cycle for each new feature or component:\n\nWrite tests that specify the component’s desired behavior.\nThe tests will initially fail because the component does not yet exist.\nCreate the minimal implementation that passes the test.\nRefactor the code to meet design standards, running the tests with each change to ensure correctness."
  },
  {
    "objectID": "slides/unit-tests.html#section",
    "href": "slides/unit-tests.html#section",
    "title": "Unit tests",
    "section": "",
    "text": "Why work this way?\n\nWriting the tests may help you realize\n\nwhat arguments the function must take,\n\nwhat other data it needs,\n\nand what kinds of errors it needs to handle.\n\nThe tests define a specific plan for what the function must do.\nYou will catch bugs at the beginning instead of at the end (or never).\nTesting is part of design, instead of a lame afterthought you dread doing."
  },
  {
    "objectID": "slides/unit-tests.html#rules-of-thumb",
    "href": "slides/unit-tests.html#rules-of-thumb",
    "title": "Unit tests",
    "section": "Rules of thumb",
    "text": "Rules of thumb\nKeep tests in separate files\nfrom the code they test. This makes it easy to run them separately.\nGive tests names.\nTesting frameworks usually let you give the test functions names or descriptions. test_1 doesn’t help you at all, but test_tree_insert makes it easy for you to remember what the test is for.\nMake tests replicable.\nIf a test involves random data, what do you do when the test fails? You need some way to know what random values it used so you can figure out why the test fails."
  },
  {
    "objectID": "slides/unit-tests.html#rules-of-thumb-1",
    "href": "slides/unit-tests.html#rules-of-thumb-1",
    "title": "Unit tests",
    "section": "Rules of thumb",
    "text": "Rules of thumb\nUse tests instead of the REPL.\nIf you’re building a complicated function, write the tests in advance and use them to help you while you write the function. You’ll waste time calling over and over at the REPL.\nAvoid testing against another’s code/package.\nYou don’t know the ins and outs of what they do. If they change the code, your tests will fail.\nTest Units, not main functions.\nYou should write small functions that do one thing. Test those. Don’t write one huge 1000-line function and try to test that.\nAvoid random numbers.\nSeeds are not always portable."
  },
  {
    "objectID": "slides/unit-tests.html#assertions",
    "href": "slides/unit-tests.html#assertions",
    "title": "Unit tests",
    "section": "Assertions",
    "text": "Assertions\nAssertions are things that must be true. Failure means “Quit”.\n\nThere’s no way to recover.\nThink: passed in bad arguments.\n\n\ndef fit(data, ...):\n\n    for it in range(max_iterations):\n        # iterative fitting code here\n        ...\n\n        # Plausibility check\n        assert np.all(alpha >= 0), \"negative alpha\"\n        assert np.all(theta >= 0), \"negative theta\"\n        assert omega > 0, \"Nonpositive omega\"\n        assert eta2 > 0, \"Nonpositive eta2\"\n        assert sigma2 > 0, \"Nonpositive sigma2\"\n\n    ...\n\nThe parameters have to be positive. Negative is impossible. No way to recover."
  },
  {
    "objectID": "slides/unit-tests.html#errors",
    "href": "slides/unit-tests.html#errors",
    "title": "Unit tests",
    "section": "Errors",
    "text": "Errors\nErrors are for unexpected conditions that could be handled by the calling code.\n\nYou could perform some action to work around the error, fix it, or report it to the user.\n\nExample:\n\nI give you directions to my house. You get lost. You could recover.\nMaybe retrace your steps, see if you missed a sign post.\nMaybe search on Google Maps to locate your self in relation to a landmark.\nIf those fail, message me.\nIf I don’t respond, get an Uber.\nFinally, give up and go home."
  },
  {
    "objectID": "slides/unit-tests.html#errors-1",
    "href": "slides/unit-tests.html#errors-1",
    "title": "Unit tests",
    "section": "Errors",
    "text": "Errors\nCode can also do this. It can try the function and catch errors to recover automatically.\nFor example:\n\nLoad some data from the internet. If the file doesn’t exist, create some.\nRun some iterative algorithm. If we haven’t converged, restart from another place.\n\nCode can fix errors without user input. It can’t fix assertions."
  },
  {
    "objectID": "slides/unit-tests.html#best-practices",
    "href": "slides/unit-tests.html#best-practices",
    "title": "Unit tests",
    "section": "Best practices",
    "text": "Best practices\n\n\nDo this\n\nfoo <- function(x) {\n  if (x < 0) stop(x, \" is not positive\")\n}\n\nfoo <- function(x) {\n  if (x < 0) message(x, \" is not positive\")\n  # not useful unless we fix it too...\n}\n\nfoo <- function(x) {\n  if (x < 0) warning(x, \" is not positive\")\n  # not useful unless we fix it too...\n}\n\nfoo <- function(x) {\n  if (length(x) == 0)\n    rlang::abort(\"no data\", class=\"no_input_data\")\n}\n\nThese allow error handling.\n\nDon’t do this\n\nfoo <- function(x) {\n  if (x < 0) {\n    print(paste0(x, \" is not positive\"))\n    return(NULL)\n  }\n  ...\n}\n\nfoo <- function(x) {\n  if (x < 0) cat(\"uh oh.\")\n  ...\n}\n\nCan’t recover.\nDon’t know what went wrong."
  },
  {
    "objectID": "slides/unit-tests.html#practice",
    "href": "slides/unit-tests.html#practice",
    "title": "Unit tests",
    "section": "Practice",
    "text": "Practice\nGradient ascent.\n\nSuppose we want to find \\(\\max_x f(x)\\).\nWe repeat the update \\(x \\leftarrow x + \\gamma f'(x)\\) until convergence, for some \\(\\gamma > 0\\).\n\nPoisson likelihood.\n\nRecall the likelihood: \\(L(\\lambda; y_1,\\ldots,y_n) = \\prod_{i=1}^n \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!}\\)\n\nGoal: find the MLE for \\(\\lambda\\) using gradient ascent"
  },
  {
    "objectID": "slides/unit-tests.html#deliverables-2-r-scripts",
    "href": "slides/unit-tests.html#deliverables-2-r-scripts",
    "title": "Unit tests",
    "section": "Deliverables, 2 R scripts",
    "text": "Deliverables, 2 R scripts\n\nA function that evaluates the log likelihood. (think about sufficiency, ignorable constants)\nA function that evaluates the gradient of the log likelihood.\nA function that does the optimization.\n\nShould take in data, the log likelihood and the gradient.\nUse the loglikelihood to determine convergence.\nPass in any other necessary parameters with reasonable defaults.\n\nA collection of tests that make sure your functions work.\n\n\\[\n\\begin{aligned}\nL(\\lambda; y_1,\\ldots,y_n) &= \\prod_{i=1}^n \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!}\\\\\nx &\\leftarrow x + \\gamma f'(x)\n\\end{aligned}\n\\]"
  }
]