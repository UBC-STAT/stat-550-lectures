{
  "hash": "2e8fe82b2b0a0e932ec57a2b93084dc7",
  "result": {
    "markdown": "---\ntitle: \"Time series, a whirlwind\"\nformat: revealjs\nknitr: \n  opts_chunk: \n    cache: true\n---\n\n\n\n\n## The general linear process\n\n* Imagine that there is a noise process\n\n$$\\epsilon_j \\sim \\textrm{N}(0, 1),\\ \\textrm{i.i.d.}$$\n\n* At time $i$, we observe the sum of all past noise\n\n$$y_i = \\sum_{j=-\\infty}^0 a_{i+j} \\epsilon_j$$\n\n* Without some conditions on $\\{a_k\\}_{k=-\\infty}^0$ this process will \"run away\"\n* The result is \"non-stationary\" and difficult to analyze.\n* Stationary means (roughly) that the marginal distribution of $y_i$ does not change with $i$.\n\n## Chasing stationarity\n\n\n::: {.cell hash='time-series_cache/revealjs/unnamed-chunk-1_6ffa35a74d509b91c6ce299ee24dfed3'}\n\n```{.r .cell-code}\nn <- 1000\nset.seed(12345)\nnseq <- 5\ngenerate_ar <- function(id, n, b) {\n  y <- double(n)\n  y[1] <- rnorm(1)\n  for (i in 2:n) y[i] <- b * y[i - 1] + rnorm(1)\n  tibble(time = 1:n, y = y, id = id)\n}\nstationary <- map_dfr(1:nseq, ~ generate_ar(.x, n = n, b = .99))\nnon_stationary <- map_dfr(1:nseq, ~ generate_ar(.x, n = n, b = 1.01))\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='time-series_cache/revealjs/unnamed-chunk-2_63defcfeb1a8516a5be3d1485626fb58'}\n::: {.cell-output-display}\n![](time-series_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\n## Uses of stationarity\n\n* Lots of types (weak, strong, in-mean, wide-sense,...)\n* not required for modelling / forecasting\n* But assuming stationarity gives some important guarantees\n* Usually work with stationary processes\n\n## Standard models {.smaller}\n\n### AR(p)\n\nSuppose $\\epsilon_i$ are i.i.d. N(0, 1) (distn is convenient, but not required)\n\n$$y_i = \\mu + a_1 y_{i-1} + \\cdots + a_p y_{i-p} + \\epsilon_i$$\n\n* This is a special case of the general linear process\n* You can recursively substitute this defn into itself to get that equation\n\nEasy to estimate the `a`'s given a realization. \n\n\n::: {.cell hash='time-series_cache/revealjs/unnamed-chunk-3_5c4d7d14513b8eeb69692b46ce5025a9'}\n\n```{.r .cell-code}\ny <- arima.sim(list(ar = c(.7, -.1)), n = 1000)\nY <- y[3:1000]\nX <- cbind(lag1 = y[2:999], lag2 = y[1:998])\nsummary(lm(Y ~ X + 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Y ~ X + 0)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6164 -0.6638  0.0271  0.6456  3.8367 \n\nCoefficients:\n      Estimate Std. Error t value Pr(>|t|)    \nXlag1  0.66931    0.03167  21.134   <2e-16 ***\nXlag2 -0.04856    0.03167  -1.533    0.126    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9899 on 996 degrees of freedom\nMultiple R-squared:  0.4085,\tAdjusted R-squared:  0.4073 \nF-statistic:   344 on 2 and 996 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## AR(p)\n\n* The estimate isn't [that]{.secondary} accurate because the residuals (not the $\\epsilon$'s) are correlated. \n* (Usually, you get `1/n` convergence, here you don't.)\n* Also, this isn't the MLE. The likelihood includes $p(y_1)$, $p(y_2 | y_1)$ which `lm()` ignored.\n* The Std. Errors are unjustified.\n* But that was easy to do.\n* The correct way is\n\n\n::: {.cell hash='time-series_cache/revealjs/unnamed-chunk-4_6f2097b1b1d750be0eac51c9a6da3fb2'}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\narima(x = y, order = c(2, 0, 0), include.mean = FALSE)\n\nCoefficients:\n         ar1      ar2\n      0.6686  -0.0485\ns.e.  0.0316   0.0316\n\nsigma^2 estimated as 0.9765:  log likelihood = -1407.34,  aic = 2820.67\n```\n:::\n:::\n\n\n* The resulting estimates and SEs are identical, AFAICS.\n\n## MA(q)\n\nStart with the general linear process, but truncate the infinite sum.\n\n$$y_i = \\sum_{j=-q}^0 a_{i+j} \\epsilon_j$$\n\n* This is termed a \"moving average\" process.\n* though $a_0 + \\cdots a_{-q}$ don't sum to 1.\n* Can't write this easily as a `lm()`\n\n\n::: {.cell hash='time-series_cache/revealjs/unnamed-chunk-5_40a116039422e68a0129e2caf6236ddd'}\n\n```{.r .cell-code}\ny <- arima.sim(list(ma = c(.9, .6, .1)), n = 1000)\narima(y, c(0, 0, 3), include.mean = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\narima(x = y, order = c(0, 0, 3), include.mean = FALSE)\n\nCoefficients:\n         ma1     ma2     ma3\n      0.9092  0.6069  0.1198\ns.e.  0.0313  0.0380  0.0311\n\nsigma^2 estimated as 0.8763:  log likelihood = -1353.41,  aic = 2714.82\n```\n:::\n:::\n\n\n## MA(q) as an AR(1) hidden process\n\nLet $X_j = [\\epsilon_{j-1},\\ \\ldots,\\  \\epsilon_{j-q}]$ and write\n\n$$\n\\begin{aligned}\nX_i &= \\begin{bmatrix} a_{i-1} & a_{i-2} & \\cdots & a_{i-q}\\\\ 1 & 0 & \\cdots & 0\\\\ & & \\ddots \\\\ 0 & 0 & \\cdots & 1\\end{bmatrix} X_{i-1} + \n\\begin{bmatrix} a_{i}\\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\epsilon_i\\\\\ny_i &= \\begin{bmatrix} 1 & 0 & \\cdots 0 \\end{bmatrix} X_i\n\\end{aligned}\n$$\n\n* Now $X$ is a $q$-dimensional AR(1) (but we don't see it)\n* $y$ is deterministic conditional on $X$\n* This is the usual way these are estimated using a State-Space Model\n* Many time series models have multiple equivalent representations\n\n## AR[I]{.secondary}MA\n\n* We've been using `arima()` and `arima.sim()`, so what is left?\n* The \"I\" means \"integrated\"\n* If, for example, we can write $z_i = y_i - y_{i-1}$ and $z$ follows an ARMA(p, q), we say $y$ follows an ARIMA(p, 1, q).\n* The middle term is the degree of differencing\n\n## Other standard models\n\nSuppose we can write \n\n$$\ny_i = T_i + S_i + W_i\n$$\n\nThis is the \"classical\" decomposition of $y$ into a Trend + Seasonal + Noise.\n\nYou can estimate this with a \"Basic Structural Time Series Model\" using `StrucTS()`.\n\nA related, though slightly different model is called the STL decomposition, estimated with `stl()`.\n\nThis is \"Seasonal Decomposition of Time Series by Loess\"\n\n(LOESS is \"locally estimated scatterplot smoothing\" named/proposed independently by Bill Cleveland though originally proposed about 15 years earlier and called the Savitsky-Golay Filter)\n\n## Quick example\n\n\n::: {.cell layout-align=\"center\" hash='time-series_cache/revealjs/unnamed-chunk-6_d97b9f35fac8193dadc47553fcbe08f3'}\n\n```{.r .cell-code}\nsts <- StructTS(AirPassengers)\nbc <- stl(AirPassengers, \"periodic\") # use sin/cos to represent the seasonal\ntibble(time = seq(as.Date(\"1949-01-01\"), as.Date(\"1960-12-31\"), by = \"month\"),\n       AP = AirPassengers, StrucTS = fitted(sts)[,1], STL = rowSums(bc$time.series[,1:2])) %>%\n  pivot_longer(-time) %>%\n  ggplot(aes(time, value, color = name)) +\n  geom_line() +\n  theme_bw(base_size = 24) +\n  scale_color_viridis_d(name = \"\") +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](time-series_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Generic state space model\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n<br><br><br>\n\n$$\\begin{aligned} x_k &\\sim p(x_k | x_{k-1}) \\\\ y_k &\\sim p(y_k | x_k)\\end{aligned}$$\n\n:::\n::: {.column width=\"50%\"}\n\n* $x_k$ is unobserved, dimension $n$\n\n* $y_k$ is observed, dimension $m$\n\n* $x$ process is the [transition]{.tertiary} or [process]{.tertiary} equation\n\n* $y$ is the [observation]{.tertiary} or [measurement]{.tertiary} equation\n\n* Both are probability distributions that can depend on parameters $\\theta$\n\n* For now, assume $\\theta$ is [KNOWN]{.secondary}\n\n* We can allow the densities to vary with time.\n\n:::\n::::\n\n\n\n## GOAL(s)\n\n1. Filtering: given observations, find $$p(x_k | y_1,\\ldots y_k)$$\n\n1. Smoothing: given observations, find $$p(x_k | y_1,\\ldots y_T), \\;\\;\\ k < T$$\n\n1. Forecasting: given observations, find $$p(y_{k+1} | y_1,\\ldots,y_k)$$\n\n\n## Using Bayes Rule\n\nAssume $p(x_0)$ is known\n\n$$\n\\begin{aligned}\np(y_1,\\ldots,y_T\\ |\\ x_1, \\ldots, x_T) &= \\prod_{k=1}^T p(y_k | x_k)\\\\\np(x_0,\\ldots,x_T) &= p(x_0) \\prod_{k=1}^T p(x_k | x_{k-1})\\\\\np(x_0,\\ldots,x_T\\ |\\ y_1,\\ldots,y_T) &= \\frac{p(y_1,\\ldots,y_T\\ |\\ x_1, \\ldots, x_T)p(x_0,\\ldots,x_T)}{p(y_1,\\ldots,y_T)}\\\\ &\\propto p(y_1,\\ldots,y_T\\ |\\ x_1, \\ldots, x_T)p(x_0,\\ldots,x_T)\\end{aligned}\n$$\n\nIn principle, if things are nice, you can compute this posterior (thinking of $x$ as unknown parameters)\n\nBut in practice, computing a big multivariate posterior like this is computationally ill-advised.\n\n\n\n## Generic filtering\n\n* Recursively build up $p(x_k | y_1,\\ldots y_k)$.\n\n* Why? Because if we're collecting data in real time, this is all we need to make forecasts for future data.\n\n$$\\begin{aligned} &p(y_{T+1} | y_1,\\ldots,y_T)\\\\ &= p(y_{T+1} | x_{T+1}, y_1,\\ldots,y_T)\\\\ &= p(y_{T+1} | x_{T+1} )p(x_{T+1} | y_1,\\ldots,y_T)\\\\ &= p(y_{T+1} | x_{T+1} )p(x_{T+1} | x_T) p(x_T | y_1,\\ldots,y_T)\\end{aligned}$$\n\n* Can continue to iterate if I want to predict $h$ steps ahead\n\n$$\\begin{aligned} &p(y_{T+h} | y_1,\\ldots,y_T)= p(y_{T+h} | x_{T+h} )\\prod_{j=0}^{h-1} p(x_{T+j+1} | x_{T+j}) p(x_T | y_1,\\ldots,y_T)\\end{aligned}$$\n\n\n\n## The filtering recursion\n\n1. Initialization. Fix $p(x_0)$.\n\nIterate the following for $k=1,\\ldots,T$:\n\n2. Predict. $$p(x_k | y_{k-1}) = \\int p(x_k | x_{k-1}) p(x_{k-1} | y_1,\\ldots, y_{k-1})dx_{k-1}.$$\n\n3. Update. $$p(x_k | y_1,\\ldots,y_k) = \\frac{p(y_k | x_k)p(x_k | y_1,\\ldots,y_{k-1})}{p(y_1,\\ldots,y_k)}$$\n\n\nIn general, this is somewhat annoying because these integrals may be challenging to solve.\n\nBut with some creativity, we can use Monte Carlo for everything.\n\n\n\n## What if we make lots of assumptions?\n\nAssume that $$\\begin{aligned}p(x_0) &= N(m_0, P_0) \\\\ p_k(x_k\\ |\\ x_{k-1}) &= N(A_{k-1}x_{k-1},\\ Q_{k-1})\\\\ p_k(y_k\\ |\\ x_k) &= N(H_k x_k,\\ R_k)\\end{aligned}.$$\n\nThen [all the ugly integrals have closed-form representations]{.secondary} by properties of conditional Gaussian distributions.\n\n## Closed-form representations\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\nDistributions:\n\n$$\n\\begin{aligned}\np(x_k | y_1,\\ldots,y_{k-1}) &= N(m^{-}_k, P^{-}_k)\\\\\np(x_k | y_1,\\ldots,y_{k}) &= N(m_k, P_k)\\\\\np(y_{k} | y_1,\\ldots,y_{k-1}) &= N(H_k m^-_k, S_k)\\\\\n\\end{aligned}\n$$\nPrediction:\n$$\n\\begin{aligned}\nm^-_k &= A_{k-1}m_{k-1}\\\\\nP^-_k &= A_{k-1}P_{k-1}A^\\mathsf{T}_{k-1} + Q_{k-1}\n\\end{aligned}\n$$\n\n:::\n::: {.column width=\"50%\"}\n\nUpdate:\n$$\n\\begin{aligned}\nv_k &= y_k - H_k m_k^-\\\\\nS_k &= H_k P_k^- H_k^\\mathsf{T} + R_k\\\\\nK_k &= P^-_k H_k^\\mathsf{T} S_k^{-1}\\\\\nm_k &= m^-_k + K_{k}v_{k}\\\\\nP_k &= P^-_k - K_k S_k K_k^\\mathsf{T}\n\\end{aligned}\n$$\n\n:::\n::::\n\n\n\n## Code or it isn't real (Kalman Filter)\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell hash='time-series_cache/revealjs/kalman-filter_92f068993b3a8e47da9e73680f6cd594'}\n\n```{.r .cell-code}\nkalman <- function(y, m0, P0, A, Q, H, R) {\n  n <- length(y)\n  m <- double(n+1)\n  P <- double(n+1)\n  m[1] <- m0\n  P[1] <- P0\n  for (k in seq(n)) {\n    mm <- A * m[k]\n    Pm <- A * P[k] * A + Q\n    v <- y[k] - H * mm\n    S <- H * Pm * H + R\n    K <- Pm * H / S\n    m[k+1] <- mm + K * v\n    P[k+1] <- Pm - K * S * K\n  }\n  tibble(t = 1:n, m = m[-1], P = P[-1])\n}\n\nset.seed(2022-06-01)\nx <- double(100)\nfor (k in 2:100) x[k] = x[k - 1] + rnorm(1)\ny <- x + rnorm(100, sd = 1)\nkf <- kalman(y, 0, 5, 1, 1, 1, 1)\n```\n:::\n\n\n:::\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell layout-align=\"center\" hash='time-series_cache/revealjs/plot-it_148dfd48b73002b1b4905d52eb0f31d2'}\n::: {.cell-output-display}\n![](time-series_files/figure-revealjs/plot-it-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n:::\n::::\n\n\n## Important notes\n\n* So far, we assumed all parameters were known.\n* In reality, we had 6: `m0, P0, A, Q, H, R`\n* I sort of also think of `x` as \"parameters\" in the Bayesian sense\n* By that I mean, \"latent variables for which we have prior distributions\"\n* What if we want to estimate them?\n\n[Bayesian way]{.tertiary}: `m0` and `P0` are already the parameters of for the prior on `x1`. Put priors on the other 4.\n\n[Frequentist way]{.tertiary}: Just maximize the likelihood. Can technically take \n`P0` $\\rightarrow\\infty$ to remove it and `m0`\n\n. . .\n\nThe Likelihood is produced as a by-product of the Kalman Filter. \n\n$$-\\ell(\\theta) = \\sum_{k=1}^T \\left(v_k^\\mathsf{T}S_k^{-1}v_k + \\log |S_k| + m \\log 2\\pi\\right)$$\n\n\n\n## Smoothing \n\n* We also want $p(x_k | y_1,\\ldots,y_{T})$\n* Filtering went \"forward\" in time. At the end we got, \n$p(x_T | y_1,\\ldots,y_{T})$. Smoothing starts there and goes \"backward\"\n* For \"everything linear Gaussian\", this is again \"easy\"\n* Set $m_T^s = m_T$, $P_T^s = P_T$. \n* For $k = T-1,\\ldots,1$, \n\n\n$$\\begin{aligned}\nG_k &= P_k A_k^\\mathsf{T} [P_{k+1}^-]^{-1}\\\\\nm_k^s &= m_k + G_k(m_{k+1}^s - m_{k+1}^-)\\\\\nP_k^s &= P_k + G_k(P_{k+1}^s - P_{k+1}^-)G_k^\\mathsf{T}\\\\\nx_k | y_1,\\ldots,y_T &= N(m^s_k, P_k^s)\n\\end{aligned}$$\n\n\n## Comparing the filter and the smoother\n\n* Same data, different code (using a package)\n\n\n::: {.cell hash='time-series_cache/revealjs/kalman-smoothing_ccceb6e0c50936da76aff1a4b2891dab'}\n\n```{.r .cell-code}\nlibrary(FKF)\nfilt <- fkf(\n  a0 = 0, P0 = matrix(5), dt = matrix(0), ct = matrix(0), \n  Tt = matrix(1), Zt = matrix(1), HHt = matrix(1), GGt = matrix(1), \n  yt = matrix(y, ncol = length(y)))\nsmo <- fks(filt)\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='time-series_cache/revealjs/plot-smooth_6ca57c1d680d726f2dc16c5fae7b99c5'}\n::: {.cell-output-display}\n![](time-series_files/figure-revealjs/plot-smooth-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## What about non-linear and/or non-Gaussian\n\n$$\\begin{aligned} x_k &\\sim p(x_k | x_{k-1}) \\\\ y_k &\\sim p(y_k | x_k)\\end{aligned}$$\n\nThen we need to solve integrals. This is a pain. We approximate them. \n\nThese all give [approximations to the filtering distribution]{.secondary}\n\n* Extended Kalman filter - basically do a Taylor approximation, then do Kalman like\n* Uncented Kalman filter - Approximate integrals with Sigma points\n* Particle filter - Sequential Monte Carlo\n* Bootstrap filter (simple version of SMC)\n* Laplace Gaussian filter - Do a Laplace approximation to the distributions\n\n\n## The bootstrap filter\n\n* Need to **simulate** from the transition distribution (`rtrans`)\n* Need to **evaluate** the observation distribution (`dobs`)\n\n\n::: {.cell hash='time-series_cache/revealjs/bootstrap-filter_bec4a7eae231c7447bd05087a76a9dec'}\n\n```{.r .cell-code}\nboot_filter <- \n  function(y, B = 1000, rtrans, dobs, a0 = 0, P0 = 1, perturb = function(x) x) {\n    n <- length(y)\n    filter_est <- matrix(0, n, B)\n    predict_est <- matrix(0, n, B)\n    init <- rnorm(B, a0, P0)\n    filter_est[1, ] = init\n    for (i in seq(n)) {\n      raw_w <- dobs(y[i], filter_est[i, ])\n      w <- raw_w / sum(raw_w)\n      selection <- sample.int(B, replace = TRUE, prob = w)\n      filter_est[i, ] <- perturb(filter_est[i, selection])\n      predict_est[i, ] <- rtrans(filter_est[i, ])\n      if (i < n) filter_est[i + 1, ] <- predict_est[i, ]\n    }\n    list(filt = filter_est, pred = predict_est)\n  }\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}