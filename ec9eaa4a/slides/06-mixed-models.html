<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>06 Mixed models and missingness</title>
    <meta charset="utf-8" />
    <meta name="author" content="STAT 550" />
    <meta name="author" content="Daniel J. McDonald" />
    <script src="assets/libs/header-attrs/header-attrs.js"></script>
    <script src="https://kit.fontawesome.com/ae71192e04.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="assets/xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="assets/slides-style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# 06 Mixed models and missingness
### STAT 550
### Daniel J. McDonald
### Last modified - 2022-02-17

---




## Setting

Most theoretical results are for iid data 

All the `lm(...)` / `glm(...)` stuff you learned is for iid

Another common name for this type of data is "cross-sectional"

Lots of data is not this

--

&lt;hr&gt;

1. **Longitudinal (panel) data:** measure `\(M\)` units repeatedly at `\(T\)` times

2. **Clustered data:** measure `\(M\)` units at each of `\(J\)` locations

3. **Time series:** measure 1 unit at `\(T\)` times

--

These slides are **not** about time series.


---

## Examples


.pull-left-narrow[

1. Quiz scores

2. HIV data

3. TSR Shifts?

4. Sugary beverages?
]

.pull-right-wide[


![](rmd_gfx/06-mixed-models/bc-covdi-plot-1.svg)&lt;!-- --&gt;
]

---

## Clustered or Longitudinal data

* Just correlated within the unit

&gt; Within each unit, the observations are correlated.


* Correlated over time

&gt; Within each location, past values are related to future values.


* Random effects

&gt; Each unit / location needs it's own mean parameters.

--

 ---
 
$$
\newcommand{\x}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vbeta}{\boldsymbol{\beta}}
\newcommand{\N}{\textrm{N}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\vgam}{\boldsymbol{\gamma}}
\newcommand{\veps}{\boldsymbol{\epsilon}}
$$

**Implications**: (i-th observation in the j-th unit)

`$$y_{ij} = \x_{ij}^\top \vbeta + \epsilon_{ij}, \quad \veps_j \overset{iid}{\sim} \N(0, \mathbf{D}_j)$$`

`$$y_{ij} = \x_{ij}^\top \vbeta + \epsilon_{ij}, \quad \veps_j \overset{iid}{\sim} \N(0, \mathbf{D}_j),\quad \mathbf{D}_j \textrm{ structured}$$`

`$$y_{ij} = \x_{ij}^\top \vbeta + \z_{ij}^\top \vgam_j  + \epsilon_{ij}, \quad \veps_j \overset{iid}{\sim} \N(0, \mathbf{D}_j),\quad \mathbf{D}_j \textrm{ structured,}\quad \vgam_j \overset{iid}{\sim}\N(0,\mathbf{R})$$`

---

## Notes so far

1. Lots of parameters 

2. Impacts coefficient estimates, model specification

3. Impacts inference (CIs on the coefficients)

--

 ---
 
Need assumptions to make this work (usually)

**Simple example:** (random effects model)

* Observe `\(M\)` students in `\(J\)` classrooms. Each student takes a standardized test.

* `\(y_{ij}\)` is the score of the ith student in the jth classroom.

* Typical (random effects) model is:
`$$y_{ij} = \beta_0 + \gamma_j + \epsilon_{ij},\quad \epsilon_{ij} \overset{iid}{\sim} \N(0,\sigma^2),\quad \gamma_j \overset{iid}{\sim} \N(0,\tau^2)$$`

--

**Compare to:**

`$$y_{ij} = \beta_0 + \x_{ij}^\top\vbeta + \epsilon_{ij},\quad \epsilon_{ij} \overset{iid}{\sim} \N(0,\sigma^2)$$`

where `\(\x_{ij}\)` is a 0-1 dummy vector indicating membership in class `\(j\)`

---
## Watch out

`$$y_{ij} = \beta_0 + \gamma_j + \epsilon_{ij},\quad \epsilon_{ij} \overset{iid}{\sim} \N(0,\sigma^2),\quad \gamma_j \overset{iid}{\sim} \N(0,\tau^2)$$`

On average, the `\(\gamma_j\)`'s are zero. But their estimates won't necessarily sum to 0 exactly.

However, they are interpreted as deviations from `\(\beta_0\)`, the overall average score.

`\(\textrm{Var}(y_{ij}) = \sigma^2 + \tau^2\)` for all `\(i,j\)`.

We have also modeled correlation within each classroom.

--

* `\(\textrm{Cov}(y_{ij},\ y_{i'j}) = \tau^2 &gt;0\)`

--

**Compare to**
`$$y_{ij} = \x_{ij}^\top\vbeta + \epsilon_{ij},\quad \epsilon_{ij} \overset{iid}{\sim} \N(0,\sigma^2),$$`
where `\(\x_{ij}^\top = \mathbf{1}(\textrm{person i is in class j})\)`.

* ~Won't change the estimated variance of `\(y_{ij}\)`.~ Think about what `\(y_{ij}\)` is.
* But no model for the covariance within schools
* Interpretation of the coefficients is different (could address with a contrast)

---

## Likelihood

`$$L(\beta_0,\ \vgam,\ \sigma^2,\ \tau^2 ;\ \mathbf{y}) = p(\mathbf{y}\ |\ \vgam)p(\vgam) = \prod_{i=1}^M\prod_{j=1}^J \frac{1}{\sigma\tau}\varphi\left(\frac{y_{ij}-(\beta_0 +\vgam_j)}{\sigma}\right)\varphi\left(\frac{\vgam_j}{\tau}\right).$$`

MLEs are easy to calculate for everything, here.

Not easy in general.

--

`$$\ell(\theta; \vy) \propto \frac{1}{2MJ}\lVert \vy -\beta_0 -  \mathbf{X}\vgam\rVert_2^2 + \frac{\sigma^2}{2\tau^2}\lVert \vgam \rVert_2^2$$`

Easy to get a point estimate with ridge regression (have to choose `\(\lambda = \sigma^2/\tau^2\)`)

---

## Bayesian version

`$$y_{ij} = \beta_0 + \gamma_j + \epsilon_{ij},\quad \epsilon_{ij} \overset{iid}{\sim} \N(0,\sigma^2),\quad \gamma_j \overset{iid}{\sim} \N(0,\tau^2)$$`

Need priors on `\(\beta_0\)` (fixed effect?), `\(\sigma^2\)`, `\(\tau^2\)`.

Can do uninformative, everything is nice and conjugate.

Looks "like" the ridge version.

---

## Back to full generality

`$$y_{ij} = \x_{ij}^\top \vbeta + \z_{ij}^\top \vgam_j  + \epsilon_{ij}, \quad \epsilon_{ij} \overset{iid}{\sim} \N(0, \sigma^2),\quad \vgam_j \overset{iid}{\sim}\N(0,\mathbf{R})$$`

The `\(\vbeta\)` is the "fixed" effect.

The `\(\vgam\)` is the "random" effect.

Both together are a "mixed model".

Easily estimated in `R` with `lme4::lmer(...)`.

This will give you p-values with frequentist coverage.

--
 
 ---

This is intellectually challenging for me.

I'm a pragmatic frequentist. 

I've decided `\(\vgam_j\)` is random, so why not just do the Bayesian thing now?

* You don't get a p-value.... (turns out you don't from `{lme4}` either)

---
## Worked example


```r
library(lme4)
str(sleepstudy)
```

```
## 'data.frame':	180 obs. of  3 variables:
##  $ Reaction: num  250 259 251 321 357 ...
##  $ Days    : num  0 1 2 3 4 5 6 7 8 9 ...
##  $ Subject : Factor w/ 18 levels "308","309","310",..: 1 1 1 1 1 1 1 1 1 1 ...
```

* On day 0 the subjects had their normal amount of sleep. 
* Starting that night they were restricted to 3 hours of sleep per night. 
* `Reaction`, represents average reaction times in milliseconds (ms) on a series of tests given each `Day` to each `Subject`

![](rmd_gfx/06-mixed-models/unnamed-chunk-2-1.svg)&lt;!-- --&gt;

---

## Two models

Random intercept, fixed slope.


```r
f1 &lt;- lmer(Reaction ~ (1 | Subject) + Days, sleepstudy)
```

Random intercept, random slope.


```r
f2 &lt;- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
```

Comparisons


```r
anova(f1, f2)
```

```
## Data: sleepstudy
## Models:
## f1: Reaction ~ (1 | Subject) + Days
## f2: Reaction ~ Days + (Days | Subject)
##    npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)    
## f1    4 1802.1 1814.8 -897.04   1794.1                         
## f2    6 1763.9 1783.1 -875.97   1751.9 42.139  2  7.072e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---

## Inference

* `{lme4}` does not produce p-values [see the vignette](https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf)

* `{lme4}` also does not produce CI's or p-values for the random effects.


```r
confint(f2)
```

```
##                   2.5 %      97.5 %
## .sig01       14.3814379  37.7159918
## .sig02       -0.4815008   0.6849863
## .sig03        3.8011643   8.7533658
## .sigma       22.8982669  28.8579965
## (Intercept) 237.6806955 265.1295145
## Days          7.3586533  13.5759188
```


```r
ranef(f2) # display the point estimates for the random effects
```

I find this annoying. 

The reasoning is something like "CI's are asymptotic, but the asymptotics don't really hold for any finite sample size, so these things aren't trustworthy"

---

## An alternative

1. Be a Bayesian. .secondary[Many clients won't want this.]

2. [`{merTools}`](https://cran.r-project.org/web/packages/merTools/vignettes/merToolsIntro.html).


```r
library(merTools)
rs &lt;- REsim(f2, n.sims = 200) # parametric bootstrap
fs &lt;- FEsim(f2, n.sims = 200)
```

.pull-left[

```r
merTools::plotFEsim(fs)
```

![](rmd_gfx/06-mixed-models/fe-plot-1.svg)&lt;!-- --&gt;
]

.pull-right[

```r
merTools::plotREsim(rs)
```

![](rmd_gfx/06-mixed-models/re-plot-1.svg)&lt;!-- --&gt;
]

---

## Ugly

I think those plots are ugly. I'll fix the random one.

1. Look at the function. Type `View(plotREsim)` at the console
2. Find the guts and see what I need to fix.

.pull-left[

```r
grabber &lt;- function(x) x[1]
p &lt;- rs %&gt;% 
  mutate(
    sd = sd * qnorm(1 - (1-.95)/2),
    ymin = median + sd,
    ymax = median - sd,
    pid = fct_reorder(as.factor(groupID), median, grabber)) %&gt;%
  ggplot(aes(pid)) +
  geom_hline(yintercept = 0) +
  geom_linerange(aes(ymin=ymin, ymax=ymax)) +
  geom_point(aes(y = median, color = pid), size = 3) +
  facet_wrap(~term, ncol=1, scales = "free_y") +
  theme_bw(18) +
  scale_color_viridis_d() +
  theme(legend.position = "none",
        axis.text.x = element_text(size = 10)) +
  ylab("Random effects")
```
]

.pull-right[
![](rmd_gfx/06-mixed-models/re-plot-better-1.svg)&lt;!-- --&gt;
]

---
class: middle, center, inverse


# Missingness

---

## Typical (na√Øve) approach

The .secondary[complete case] approach: use only the complete cases, ignore the rest.


```r
mean(z, na.rm = TRUE)
lm(y~., data=dat, na.action = na.omit)
```

The issue isn't really .secondary[existence of missing cases].

The issue is .tertiary[why] are the missing cases missing?

--

* Student skips homework because she got a headache. (Headaches are random, not correlated with homework skill)

* Student skips homework because she read the first question and it was hard.

---

## Missing data mechanisms

**Missing completely at random** (MCAR) -- the missingness depends neither on the missing values nor on the observed values


**Missing at random** (MAR) -- the missingness does not depend on the missing values but may depend on observed values

**Missing not at random** (MNAR) -- the missingness may depend on the missing values and the observed values (also called "non-ignorable missingness")

--

 ---

1. Student had a headache and skipped the assignment.

2. Student did poorly on homework 1 and skipped homework 2.

3. Student thought homework looked hard, didn't do it.

---
## Na√Øve approaches

**Complete case method:** Throw out the missing observations.

* Works if MCAR, otherwise biased. 

* Technically inefficient compared with the hypothetical that the data was complete.

**Mean imputation:** impute missing data with average of observed

**Last-value carried forward:** impute later missing values with the previous observed value

* The second is common in longitudinal / time series.

* Both result in incorrect confidence intervals.

* You treat the "filled" data as real, so sample size is larger.

* You ignore the uncertainty in the imputation

---

## Multiple imputation

.emphasis[
Impute values for each missing entry with the predicted values based on other observed data under an assumed imputation model.
]

Repeat the imputation multiple times.

Example procedure (given a matrix of data):

1. Treat column 1 as a response and the others as predictors.
2. Initialize missing values in predictors to something.
3. Fit a linear model of y on the rest. 
4. Predict missing values in y given complete rest.
5. Impute missing y with prediction + noise from prediction interval.

Loop over columns.

--

 ---
 
End up with multiple completed data sets. 

Analyze each individually with the same model.

Combine the results in a particular way.

Correctly incorporates uncertainty from missingness into CIs for final model.


---

## Related options

`{mice}` package does basically this. See the [documentation](https://amices.org/mice/)

### Others

**Be a Bayesian.**

* Not much different from `{mice}`. 

* Looks like "missing data are parameters. model them. do MCMC over all parameters"

**EM algorithm.**

* Integrates over the missingness.

* Need to derive the algorithm specifically for your problem.

* Sort of like infinitely many imputations.

**Weighting.**

* Estimating equations, observed data get larger weight.

--

 ---
 
Censoring, dropouts, measurement error are not the same as missingness.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="assets/macros.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
